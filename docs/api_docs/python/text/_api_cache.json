{
  "duplicate_of": {
    "text.BertTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.BertTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.BertTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.BertTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.BertTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.BertTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.BertTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.BertTokenizer.split": "text.Tokenizer.split",
    "text.BertTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.BertTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.Detokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Detokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Detokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Detokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Detokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Detokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Detokenizer.__new__": "text.BertTokenizer.__new__",
    "text.Detokenizer.name": "text.BertTokenizer.name",
    "text.Detokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.Detokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.Detokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.Detokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.Detokenizer.variables": "text.BertTokenizer.variables",
    "text.HubModuleSplitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.HubModuleSplitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.HubModuleSplitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.HubModuleSplitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.HubModuleSplitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.HubModuleSplitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.HubModuleSplitter.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleSplitter.name": "text.BertTokenizer.name",
    "text.HubModuleSplitter.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.HubModuleSplitter.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.HubModuleSplitter.submodules": "text.keras.layers.ToDense.submodules",
    "text.HubModuleSplitter.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.HubModuleSplitter.variables": "text.BertTokenizer.variables",
    "text.HubModuleTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.HubModuleTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.HubModuleTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.HubModuleTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.HubModuleTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.HubModuleTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.HubModuleTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleTokenizer.name": "text.BertTokenizer.name",
    "text.HubModuleTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.HubModuleTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.HubModuleTokenizer.split": "text.Tokenizer.split",
    "text.HubModuleTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.HubModuleTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.HubModuleTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.HubModuleTokenizer.variables": "text.BertTokenizer.variables",
    "text.SentencepieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SentencepieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SentencepieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SentencepieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SentencepieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SentencepieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SentencepieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SentencepieceTokenizer.name": "text.BertTokenizer.name",
    "text.SentencepieceTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.SentencepieceTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.SentencepieceTokenizer.split": "text.Tokenizer.split",
    "text.SentencepieceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.SentencepieceTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.SentencepieceTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.SentencepieceTokenizer.variables": "text.BertTokenizer.variables",
    "text.SplitMergeFromLogitsTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitMergeFromLogitsTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitMergeFromLogitsTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitMergeFromLogitsTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitMergeFromLogitsTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitMergeFromLogitsTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitMergeFromLogitsTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SplitMergeFromLogitsTokenizer.name": "text.BertTokenizer.name",
    "text.SplitMergeFromLogitsTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.SplitMergeFromLogitsTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.SplitMergeFromLogitsTokenizer.split": "text.Tokenizer.split",
    "text.SplitMergeFromLogitsTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.SplitMergeFromLogitsTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.SplitMergeFromLogitsTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.SplitMergeFromLogitsTokenizer.variables": "text.BertTokenizer.variables",
    "text.SplitMergeTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitMergeTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitMergeTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitMergeTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitMergeTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitMergeTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitMergeTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SplitMergeTokenizer.name": "text.BertTokenizer.name",
    "text.SplitMergeTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.SplitMergeTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.SplitMergeTokenizer.split": "text.Tokenizer.split",
    "text.SplitMergeTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.SplitMergeTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.SplitMergeTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.SplitMergeTokenizer.variables": "text.BertTokenizer.variables",
    "text.Splitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Splitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Splitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Splitter.__init__": "text.Detokenizer.__init__",
    "text.Splitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.Splitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Splitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Splitter.__new__": "text.BertTokenizer.__new__",
    "text.Splitter.name": "text.BertTokenizer.name",
    "text.Splitter.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.Splitter.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.Splitter.submodules": "text.keras.layers.ToDense.submodules",
    "text.Splitter.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.Splitter.variables": "text.BertTokenizer.variables",
    "text.StateBasedSentenceBreaker.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.StateBasedSentenceBreaker.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.StateBasedSentenceBreaker.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.StateBasedSentenceBreaker.__le__": "text.keras.layers.ToDense.__le__",
    "text.StateBasedSentenceBreaker.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.StateBasedSentenceBreaker.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.StateBasedSentenceBreaker.__new__": "text.BertTokenizer.__new__",
    "text.Tokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Tokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Tokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Tokenizer.__init__": "text.Detokenizer.__init__",
    "text.Tokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Tokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Tokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Tokenizer.__new__": "text.BertTokenizer.__new__",
    "text.Tokenizer.name": "text.BertTokenizer.name",
    "text.Tokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.Tokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.Tokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.Tokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.Tokenizer.variables": "text.BertTokenizer.variables",
    "text.TokenizerWithOffsets.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.TokenizerWithOffsets.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.TokenizerWithOffsets.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.TokenizerWithOffsets.__init__": "text.Detokenizer.__init__",
    "text.TokenizerWithOffsets.__le__": "text.keras.layers.ToDense.__le__",
    "text.TokenizerWithOffsets.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.TokenizerWithOffsets.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.TokenizerWithOffsets.__new__": "text.BertTokenizer.__new__",
    "text.TokenizerWithOffsets.name": "text.BertTokenizer.name",
    "text.TokenizerWithOffsets.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.TokenizerWithOffsets.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.TokenizerWithOffsets.split": "text.Tokenizer.split",
    "text.TokenizerWithOffsets.submodules": "text.keras.layers.ToDense.submodules",
    "text.TokenizerWithOffsets.tokenize": "text.Tokenizer.tokenize",
    "text.TokenizerWithOffsets.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.TokenizerWithOffsets.variables": "text.BertTokenizer.variables",
    "text.UnicodeCharTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.UnicodeCharTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.UnicodeCharTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.UnicodeCharTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.UnicodeCharTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.UnicodeCharTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.UnicodeCharTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.UnicodeCharTokenizer.name": "text.BertTokenizer.name",
    "text.UnicodeCharTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.UnicodeCharTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.UnicodeCharTokenizer.split": "text.Tokenizer.split",
    "text.UnicodeCharTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.UnicodeCharTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.UnicodeCharTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.UnicodeCharTokenizer.variables": "text.BertTokenizer.variables",
    "text.UnicodeScriptTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.UnicodeScriptTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.UnicodeScriptTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.UnicodeScriptTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.UnicodeScriptTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.UnicodeScriptTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.UnicodeScriptTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.UnicodeScriptTokenizer.name": "text.BertTokenizer.name",
    "text.UnicodeScriptTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.UnicodeScriptTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.UnicodeScriptTokenizer.split": "text.Tokenizer.split",
    "text.UnicodeScriptTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.UnicodeScriptTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.UnicodeScriptTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.UnicodeScriptTokenizer.variables": "text.BertTokenizer.variables",
    "text.WhitespaceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WhitespaceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WhitespaceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WhitespaceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WhitespaceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WhitespaceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WhitespaceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WhitespaceTokenizer.name": "text.BertTokenizer.name",
    "text.WhitespaceTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.WhitespaceTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.WhitespaceTokenizer.split": "text.Tokenizer.split",
    "text.WhitespaceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.WhitespaceTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.WhitespaceTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.WhitespaceTokenizer.variables": "text.BertTokenizer.variables",
    "text.WordShape.name": "text.Reduction.name",
    "text.WordShape.value": "text.Reduction.value",
    "text.WordpieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WordpieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WordpieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WordpieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WordpieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WordpieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WordpieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WordpieceTokenizer.name": "text.BertTokenizer.name",
    "text.WordpieceTokenizer.name_scope": "text.keras.layers.ToDense.name_scope",
    "text.WordpieceTokenizer.non_trainable_variables": "text.BertTokenizer.non_trainable_variables",
    "text.WordpieceTokenizer.split": "text.Tokenizer.split",
    "text.WordpieceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.WordpieceTokenizer.submodules": "text.keras.layers.ToDense.submodules",
    "text.WordpieceTokenizer.trainable_variables": "text.BertTokenizer.trainable_variables",
    "text.WordpieceTokenizer.variables": "text.BertTokenizer.variables"
  },
  "is_fragment": {
    "text": false,
    "text.BertTokenizer": false,
    "text.BertTokenizer.__eq__": true,
    "text.BertTokenizer.__ge__": true,
    "text.BertTokenizer.__gt__": true,
    "text.BertTokenizer.__init__": true,
    "text.BertTokenizer.__le__": true,
    "text.BertTokenizer.__lt__": true,
    "text.BertTokenizer.__ne__": true,
    "text.BertTokenizer.__new__": true,
    "text.BertTokenizer.name": true,
    "text.BertTokenizer.name_scope": true,
    "text.BertTokenizer.non_trainable_variables": true,
    "text.BertTokenizer.split": true,
    "text.BertTokenizer.split_with_offsets": true,
    "text.BertTokenizer.submodules": true,
    "text.BertTokenizer.tokenize": true,
    "text.BertTokenizer.tokenize_with_offsets": true,
    "text.BertTokenizer.trainable_variables": true,
    "text.BertTokenizer.variables": true,
    "text.BertTokenizer.with_name_scope": true,
    "text.Detokenizer": false,
    "text.Detokenizer.__eq__": true,
    "text.Detokenizer.__ge__": true,
    "text.Detokenizer.__gt__": true,
    "text.Detokenizer.__init__": true,
    "text.Detokenizer.__le__": true,
    "text.Detokenizer.__lt__": true,
    "text.Detokenizer.__ne__": true,
    "text.Detokenizer.__new__": true,
    "text.Detokenizer.detokenize": true,
    "text.Detokenizer.name": true,
    "text.Detokenizer.name_scope": true,
    "text.Detokenizer.non_trainable_variables": true,
    "text.Detokenizer.submodules": true,
    "text.Detokenizer.trainable_variables": true,
    "text.Detokenizer.variables": true,
    "text.Detokenizer.with_name_scope": true,
    "text.HubModuleSplitter": false,
    "text.HubModuleSplitter.__eq__": true,
    "text.HubModuleSplitter.__ge__": true,
    "text.HubModuleSplitter.__gt__": true,
    "text.HubModuleSplitter.__init__": true,
    "text.HubModuleSplitter.__le__": true,
    "text.HubModuleSplitter.__lt__": true,
    "text.HubModuleSplitter.__ne__": true,
    "text.HubModuleSplitter.__new__": true,
    "text.HubModuleSplitter.name": true,
    "text.HubModuleSplitter.name_scope": true,
    "text.HubModuleSplitter.non_trainable_variables": true,
    "text.HubModuleSplitter.split": true,
    "text.HubModuleSplitter.split_with_offsets": true,
    "text.HubModuleSplitter.submodules": true,
    "text.HubModuleSplitter.trainable_variables": true,
    "text.HubModuleSplitter.variables": true,
    "text.HubModuleSplitter.with_name_scope": true,
    "text.HubModuleTokenizer": false,
    "text.HubModuleTokenizer.__eq__": true,
    "text.HubModuleTokenizer.__ge__": true,
    "text.HubModuleTokenizer.__gt__": true,
    "text.HubModuleTokenizer.__init__": true,
    "text.HubModuleTokenizer.__le__": true,
    "text.HubModuleTokenizer.__lt__": true,
    "text.HubModuleTokenizer.__ne__": true,
    "text.HubModuleTokenizer.__new__": true,
    "text.HubModuleTokenizer.name": true,
    "text.HubModuleTokenizer.name_scope": true,
    "text.HubModuleTokenizer.non_trainable_variables": true,
    "text.HubModuleTokenizer.split": true,
    "text.HubModuleTokenizer.split_with_offsets": true,
    "text.HubModuleTokenizer.submodules": true,
    "text.HubModuleTokenizer.tokenize": true,
    "text.HubModuleTokenizer.tokenize_with_offsets": true,
    "text.HubModuleTokenizer.trainable_variables": true,
    "text.HubModuleTokenizer.variables": true,
    "text.HubModuleTokenizer.with_name_scope": true,
    "text.Reduction": false,
    "text.Reduction.MEAN": true,
    "text.Reduction.STRING_JOIN": true,
    "text.Reduction.SUM": true,
    "text.Reduction.name": true,
    "text.Reduction.value": true,
    "text.SentencepieceTokenizer": false,
    "text.SentencepieceTokenizer.__eq__": true,
    "text.SentencepieceTokenizer.__ge__": true,
    "text.SentencepieceTokenizer.__gt__": true,
    "text.SentencepieceTokenizer.__init__": true,
    "text.SentencepieceTokenizer.__le__": true,
    "text.SentencepieceTokenizer.__lt__": true,
    "text.SentencepieceTokenizer.__ne__": true,
    "text.SentencepieceTokenizer.__new__": true,
    "text.SentencepieceTokenizer.detokenize": true,
    "text.SentencepieceTokenizer.id_to_string": true,
    "text.SentencepieceTokenizer.name": true,
    "text.SentencepieceTokenizer.name_scope": true,
    "text.SentencepieceTokenizer.non_trainable_variables": true,
    "text.SentencepieceTokenizer.split": true,
    "text.SentencepieceTokenizer.split_with_offsets": true,
    "text.SentencepieceTokenizer.string_to_id": true,
    "text.SentencepieceTokenizer.submodules": true,
    "text.SentencepieceTokenizer.tokenize": true,
    "text.SentencepieceTokenizer.tokenize_with_offsets": true,
    "text.SentencepieceTokenizer.trainable_variables": true,
    "text.SentencepieceTokenizer.variables": true,
    "text.SentencepieceTokenizer.vocab_size": true,
    "text.SentencepieceTokenizer.with_name_scope": true,
    "text.SplitMergeFromLogitsTokenizer": false,
    "text.SplitMergeFromLogitsTokenizer.__eq__": true,
    "text.SplitMergeFromLogitsTokenizer.__ge__": true,
    "text.SplitMergeFromLogitsTokenizer.__gt__": true,
    "text.SplitMergeFromLogitsTokenizer.__init__": true,
    "text.SplitMergeFromLogitsTokenizer.__le__": true,
    "text.SplitMergeFromLogitsTokenizer.__lt__": true,
    "text.SplitMergeFromLogitsTokenizer.__ne__": true,
    "text.SplitMergeFromLogitsTokenizer.__new__": true,
    "text.SplitMergeFromLogitsTokenizer.name": true,
    "text.SplitMergeFromLogitsTokenizer.name_scope": true,
    "text.SplitMergeFromLogitsTokenizer.non_trainable_variables": true,
    "text.SplitMergeFromLogitsTokenizer.split": true,
    "text.SplitMergeFromLogitsTokenizer.split_with_offsets": true,
    "text.SplitMergeFromLogitsTokenizer.submodules": true,
    "text.SplitMergeFromLogitsTokenizer.tokenize": true,
    "text.SplitMergeFromLogitsTokenizer.tokenize_with_offsets": true,
    "text.SplitMergeFromLogitsTokenizer.trainable_variables": true,
    "text.SplitMergeFromLogitsTokenizer.variables": true,
    "text.SplitMergeFromLogitsTokenizer.with_name_scope": true,
    "text.SplitMergeTokenizer": false,
    "text.SplitMergeTokenizer.__eq__": true,
    "text.SplitMergeTokenizer.__ge__": true,
    "text.SplitMergeTokenizer.__gt__": true,
    "text.SplitMergeTokenizer.__init__": true,
    "text.SplitMergeTokenizer.__le__": true,
    "text.SplitMergeTokenizer.__lt__": true,
    "text.SplitMergeTokenizer.__ne__": true,
    "text.SplitMergeTokenizer.__new__": true,
    "text.SplitMergeTokenizer.name": true,
    "text.SplitMergeTokenizer.name_scope": true,
    "text.SplitMergeTokenizer.non_trainable_variables": true,
    "text.SplitMergeTokenizer.split": true,
    "text.SplitMergeTokenizer.split_with_offsets": true,
    "text.SplitMergeTokenizer.submodules": true,
    "text.SplitMergeTokenizer.tokenize": true,
    "text.SplitMergeTokenizer.tokenize_with_offsets": true,
    "text.SplitMergeTokenizer.trainable_variables": true,
    "text.SplitMergeTokenizer.variables": true,
    "text.SplitMergeTokenizer.with_name_scope": true,
    "text.Splitter": false,
    "text.Splitter.__eq__": true,
    "text.Splitter.__ge__": true,
    "text.Splitter.__gt__": true,
    "text.Splitter.__init__": true,
    "text.Splitter.__le__": true,
    "text.Splitter.__lt__": true,
    "text.Splitter.__ne__": true,
    "text.Splitter.__new__": true,
    "text.Splitter.name": true,
    "text.Splitter.name_scope": true,
    "text.Splitter.non_trainable_variables": true,
    "text.Splitter.split": true,
    "text.Splitter.submodules": true,
    "text.Splitter.trainable_variables": true,
    "text.Splitter.variables": true,
    "text.Splitter.with_name_scope": true,
    "text.StateBasedSentenceBreaker": false,
    "text.StateBasedSentenceBreaker.__eq__": true,
    "text.StateBasedSentenceBreaker.__ge__": true,
    "text.StateBasedSentenceBreaker.__gt__": true,
    "text.StateBasedSentenceBreaker.__init__": true,
    "text.StateBasedSentenceBreaker.__le__": true,
    "text.StateBasedSentenceBreaker.__lt__": true,
    "text.StateBasedSentenceBreaker.__ne__": true,
    "text.StateBasedSentenceBreaker.__new__": true,
    "text.StateBasedSentenceBreaker.break_sentences": true,
    "text.StateBasedSentenceBreaker.break_sentences_with_offsets": true,
    "text.Tokenizer": false,
    "text.Tokenizer.__eq__": true,
    "text.Tokenizer.__ge__": true,
    "text.Tokenizer.__gt__": true,
    "text.Tokenizer.__init__": true,
    "text.Tokenizer.__le__": true,
    "text.Tokenizer.__lt__": true,
    "text.Tokenizer.__ne__": true,
    "text.Tokenizer.__new__": true,
    "text.Tokenizer.name": true,
    "text.Tokenizer.name_scope": true,
    "text.Tokenizer.non_trainable_variables": true,
    "text.Tokenizer.split": true,
    "text.Tokenizer.submodules": true,
    "text.Tokenizer.tokenize": true,
    "text.Tokenizer.trainable_variables": true,
    "text.Tokenizer.variables": true,
    "text.Tokenizer.with_name_scope": true,
    "text.TokenizerWithOffsets": false,
    "text.TokenizerWithOffsets.__eq__": true,
    "text.TokenizerWithOffsets.__ge__": true,
    "text.TokenizerWithOffsets.__gt__": true,
    "text.TokenizerWithOffsets.__init__": true,
    "text.TokenizerWithOffsets.__le__": true,
    "text.TokenizerWithOffsets.__lt__": true,
    "text.TokenizerWithOffsets.__ne__": true,
    "text.TokenizerWithOffsets.__new__": true,
    "text.TokenizerWithOffsets.name": true,
    "text.TokenizerWithOffsets.name_scope": true,
    "text.TokenizerWithOffsets.non_trainable_variables": true,
    "text.TokenizerWithOffsets.split": true,
    "text.TokenizerWithOffsets.split_with_offsets": true,
    "text.TokenizerWithOffsets.submodules": true,
    "text.TokenizerWithOffsets.tokenize": true,
    "text.TokenizerWithOffsets.tokenize_with_offsets": true,
    "text.TokenizerWithOffsets.trainable_variables": true,
    "text.TokenizerWithOffsets.variables": true,
    "text.TokenizerWithOffsets.with_name_scope": true,
    "text.UnicodeCharTokenizer": false,
    "text.UnicodeCharTokenizer.__eq__": true,
    "text.UnicodeCharTokenizer.__ge__": true,
    "text.UnicodeCharTokenizer.__gt__": true,
    "text.UnicodeCharTokenizer.__init__": true,
    "text.UnicodeCharTokenizer.__le__": true,
    "text.UnicodeCharTokenizer.__lt__": true,
    "text.UnicodeCharTokenizer.__ne__": true,
    "text.UnicodeCharTokenizer.__new__": true,
    "text.UnicodeCharTokenizer.detokenize": true,
    "text.UnicodeCharTokenizer.name": true,
    "text.UnicodeCharTokenizer.name_scope": true,
    "text.UnicodeCharTokenizer.non_trainable_variables": true,
    "text.UnicodeCharTokenizer.split": true,
    "text.UnicodeCharTokenizer.split_with_offsets": true,
    "text.UnicodeCharTokenizer.submodules": true,
    "text.UnicodeCharTokenizer.tokenize": true,
    "text.UnicodeCharTokenizer.tokenize_with_offsets": true,
    "text.UnicodeCharTokenizer.trainable_variables": true,
    "text.UnicodeCharTokenizer.variables": true,
    "text.UnicodeCharTokenizer.with_name_scope": true,
    "text.UnicodeScriptTokenizer": false,
    "text.UnicodeScriptTokenizer.__eq__": true,
    "text.UnicodeScriptTokenizer.__ge__": true,
    "text.UnicodeScriptTokenizer.__gt__": true,
    "text.UnicodeScriptTokenizer.__init__": true,
    "text.UnicodeScriptTokenizer.__le__": true,
    "text.UnicodeScriptTokenizer.__lt__": true,
    "text.UnicodeScriptTokenizer.__ne__": true,
    "text.UnicodeScriptTokenizer.__new__": true,
    "text.UnicodeScriptTokenizer.name": true,
    "text.UnicodeScriptTokenizer.name_scope": true,
    "text.UnicodeScriptTokenizer.non_trainable_variables": true,
    "text.UnicodeScriptTokenizer.split": true,
    "text.UnicodeScriptTokenizer.split_with_offsets": true,
    "text.UnicodeScriptTokenizer.submodules": true,
    "text.UnicodeScriptTokenizer.tokenize": true,
    "text.UnicodeScriptTokenizer.tokenize_with_offsets": true,
    "text.UnicodeScriptTokenizer.trainable_variables": true,
    "text.UnicodeScriptTokenizer.variables": true,
    "text.UnicodeScriptTokenizer.with_name_scope": true,
    "text.WhitespaceTokenizer": false,
    "text.WhitespaceTokenizer.__eq__": true,
    "text.WhitespaceTokenizer.__ge__": true,
    "text.WhitespaceTokenizer.__gt__": true,
    "text.WhitespaceTokenizer.__init__": true,
    "text.WhitespaceTokenizer.__le__": true,
    "text.WhitespaceTokenizer.__lt__": true,
    "text.WhitespaceTokenizer.__ne__": true,
    "text.WhitespaceTokenizer.__new__": true,
    "text.WhitespaceTokenizer.name": true,
    "text.WhitespaceTokenizer.name_scope": true,
    "text.WhitespaceTokenizer.non_trainable_variables": true,
    "text.WhitespaceTokenizer.split": true,
    "text.WhitespaceTokenizer.split_with_offsets": true,
    "text.WhitespaceTokenizer.submodules": true,
    "text.WhitespaceTokenizer.tokenize": true,
    "text.WhitespaceTokenizer.tokenize_with_offsets": true,
    "text.WhitespaceTokenizer.trainable_variables": true,
    "text.WhitespaceTokenizer.variables": true,
    "text.WhitespaceTokenizer.with_name_scope": true,
    "text.WordShape": false,
    "text.WordShape.BEGINS_WITH_OPEN_QUOTE": true,
    "text.WordShape.BEGINS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_CLOSE_QUOTE": true,
    "text.WordShape.ENDS_WITH_ELLIPSIS": true,
    "text.WordShape.ENDS_WITH_EMOTICON": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_TERMINAL_PUNCT": true,
    "text.WordShape.ENDS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_TERMINAL_PUNCT": true,
    "text.WordShape.HAS_CURRENCY_SYMBOL": true,
    "text.WordShape.HAS_EMOJI": true,
    "text.WordShape.HAS_MATH_SYMBOL": true,
    "text.WordShape.HAS_MIXED_CASE": true,
    "text.WordShape.HAS_NON_LETTER": true,
    "text.WordShape.HAS_NO_DIGITS": true,
    "text.WordShape.HAS_NO_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_NO_QUOTES": true,
    "text.WordShape.HAS_ONLY_DIGITS": true,
    "text.WordShape.HAS_PUNCTUATION_DASH": true,
    "text.WordShape.HAS_QUOTE": true,
    "text.WordShape.HAS_SOME_DIGITS": true,
    "text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_TITLE_CASE": true,
    "text.WordShape.IS_ACRONYM_WITH_PERIODS": true,
    "text.WordShape.IS_EMOTICON": true,
    "text.WordShape.IS_LOWERCASE": true,
    "text.WordShape.IS_MIXED_CASE_LETTERS": true,
    "text.WordShape.IS_NUMERIC_VALUE": true,
    "text.WordShape.IS_PUNCT_OR_SYMBOL": true,
    "text.WordShape.IS_UPPERCASE": true,
    "text.WordShape.IS_WHITESPACE": true,
    "text.WordShape.name": true,
    "text.WordShape.value": true,
    "text.WordpieceTokenizer": false,
    "text.WordpieceTokenizer.__eq__": true,
    "text.WordpieceTokenizer.__ge__": true,
    "text.WordpieceTokenizer.__gt__": true,
    "text.WordpieceTokenizer.__init__": true,
    "text.WordpieceTokenizer.__le__": true,
    "text.WordpieceTokenizer.__lt__": true,
    "text.WordpieceTokenizer.__ne__": true,
    "text.WordpieceTokenizer.__new__": true,
    "text.WordpieceTokenizer.name": true,
    "text.WordpieceTokenizer.name_scope": true,
    "text.WordpieceTokenizer.non_trainable_variables": true,
    "text.WordpieceTokenizer.split": true,
    "text.WordpieceTokenizer.split_with_offsets": true,
    "text.WordpieceTokenizer.submodules": true,
    "text.WordpieceTokenizer.tokenize": true,
    "text.WordpieceTokenizer.tokenize_with_offsets": true,
    "text.WordpieceTokenizer.trainable_variables": true,
    "text.WordpieceTokenizer.variables": true,
    "text.WordpieceTokenizer.with_name_scope": true,
    "text.case_fold_utf8": false,
    "text.coerce_to_structurally_valid_utf8": false,
    "text.find_source_offsets": false,
    "text.gather_with_default": false,
    "text.greedy_constrained_sequence": false,
    "text.keras": false,
    "text.keras.layers": false,
    "text.keras.layers.ToDense": false,
    "text.keras.layers.ToDense.__call__": true,
    "text.keras.layers.ToDense.__eq__": true,
    "text.keras.layers.ToDense.__ge__": true,
    "text.keras.layers.ToDense.__gt__": true,
    "text.keras.layers.ToDense.__init__": true,
    "text.keras.layers.ToDense.__le__": true,
    "text.keras.layers.ToDense.__lt__": true,
    "text.keras.layers.ToDense.__ne__": true,
    "text.keras.layers.ToDense.__new__": true,
    "text.keras.layers.ToDense.activity_regularizer": true,
    "text.keras.layers.ToDense.add_loss": true,
    "text.keras.layers.ToDense.add_metric": true,
    "text.keras.layers.ToDense.add_weight": true,
    "text.keras.layers.ToDense.build": true,
    "text.keras.layers.ToDense.call": true,
    "text.keras.layers.ToDense.compute_dtype": true,
    "text.keras.layers.ToDense.compute_mask": true,
    "text.keras.layers.ToDense.compute_output_shape": true,
    "text.keras.layers.ToDense.compute_output_signature": true,
    "text.keras.layers.ToDense.count_params": true,
    "text.keras.layers.ToDense.dtype": true,
    "text.keras.layers.ToDense.dtype_policy": true,
    "text.keras.layers.ToDense.dynamic": true,
    "text.keras.layers.ToDense.from_config": true,
    "text.keras.layers.ToDense.get_config": true,
    "text.keras.layers.ToDense.get_weights": true,
    "text.keras.layers.ToDense.input": true,
    "text.keras.layers.ToDense.input_spec": true,
    "text.keras.layers.ToDense.losses": true,
    "text.keras.layers.ToDense.metrics": true,
    "text.keras.layers.ToDense.name": true,
    "text.keras.layers.ToDense.name_scope": true,
    "text.keras.layers.ToDense.non_trainable_weights": true,
    "text.keras.layers.ToDense.output": true,
    "text.keras.layers.ToDense.set_weights": true,
    "text.keras.layers.ToDense.submodules": true,
    "text.keras.layers.ToDense.supports_masking": true,
    "text.keras.layers.ToDense.trainable": true,
    "text.keras.layers.ToDense.trainable_weights": true,
    "text.keras.layers.ToDense.variable_dtype": true,
    "text.keras.layers.ToDense.weights": true,
    "text.keras.layers.ToDense.with_name_scope": true,
    "text.max_spanning_tree": false,
    "text.max_spanning_tree_gradient": false,
    "text.metrics": false,
    "text.metrics.rouge_l": false,
    "text.ngrams": false,
    "text.normalize_utf8": false,
    "text.normalize_utf8_with_offsets_map": false,
    "text.pad_along_dimension": false,
    "text.regex_split": false,
    "text.regex_split_with_offsets": false,
    "text.sentence_fragments": false,
    "text.sliding_window": false,
    "text.span_alignment": false,
    "text.span_overlaps": false,
    "text.viterbi_constrained_sequence": false,
    "text.wordshape": false
  },
  "py_module_names": [
    "text"
  ],
  "site_link": null
}
