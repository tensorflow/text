{
  "duplicate_of": {
    "text.BertTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.BertTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.BertTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.BertTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.BertTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.BertTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.BertTokenizer.split": "text.Tokenizer.split",
    "text.BertTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.ByteSplitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.ByteSplitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.ByteSplitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.ByteSplitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.ByteSplitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.ByteSplitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.ByteSplitter.__new__": "text.BertTokenizer.__new__",
    "text.Detokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Detokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Detokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Detokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Detokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Detokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Detokenizer.__new__": "text.BertTokenizer.__new__",
    "text.FastBertNormalizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.FastBertNormalizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.FastBertNormalizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.FastBertNormalizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.FastBertNormalizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.FastBertNormalizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.FastBertNormalizer.__new__": "text.BertTokenizer.__new__",
    "text.FastBertTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.FastBertTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.FastBertTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.FastBertTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.FastBertTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.FastBertTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.FastBertTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.FastBertTokenizer.split": "text.Tokenizer.split",
    "text.FastBertTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.FastSentencepieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.FastSentencepieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.FastSentencepieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.FastSentencepieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.FastSentencepieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.FastSentencepieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.FastSentencepieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.FastWordpieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.FastWordpieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.FastWordpieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.FastWordpieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.FastWordpieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.FastWordpieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.FastWordpieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.FastWordpieceTokenizer.split": "text.Tokenizer.split",
    "text.FastWordpieceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.FirstNItemSelector.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.FirstNItemSelector.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.FirstNItemSelector.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.FirstNItemSelector.__le__": "text.keras.layers.ToDense.__le__",
    "text.FirstNItemSelector.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.FirstNItemSelector.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.FirstNItemSelector.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleSplitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.HubModuleSplitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.HubModuleSplitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.HubModuleSplitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.HubModuleSplitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.HubModuleSplitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.HubModuleSplitter.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.HubModuleTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.HubModuleTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.HubModuleTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.HubModuleTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.HubModuleTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.HubModuleTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.HubModuleTokenizer.split": "text.Tokenizer.split",
    "text.HubModuleTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.MaskValuesChooser.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.MaskValuesChooser.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.MaskValuesChooser.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.MaskValuesChooser.__le__": "text.keras.layers.ToDense.__le__",
    "text.MaskValuesChooser.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.MaskValuesChooser.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.MaskValuesChooser.__new__": "text.BertTokenizer.__new__",
    "text.RandomItemSelector.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.RandomItemSelector.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.RandomItemSelector.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.RandomItemSelector.__le__": "text.keras.layers.ToDense.__le__",
    "text.RandomItemSelector.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.RandomItemSelector.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.RandomItemSelector.__new__": "text.BertTokenizer.__new__",
    "text.RandomItemSelector.unselectable_ids": "text.FirstNItemSelector.unselectable_ids",
    "text.RegexSplitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.RegexSplitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.RegexSplitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.RegexSplitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.RegexSplitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.RegexSplitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.RegexSplitter.__new__": "text.BertTokenizer.__new__",
    "text.RoundRobinTrimmer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.RoundRobinTrimmer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.RoundRobinTrimmer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.RoundRobinTrimmer.__le__": "text.keras.layers.ToDense.__le__",
    "text.RoundRobinTrimmer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.RoundRobinTrimmer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.RoundRobinTrimmer.__new__": "text.BertTokenizer.__new__",
    "text.RoundRobinTrimmer.trim": "text.Trimmer.trim",
    "text.SentencepieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SentencepieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SentencepieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SentencepieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SentencepieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SentencepieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SentencepieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SentencepieceTokenizer.split": "text.Tokenizer.split",
    "text.SentencepieceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.ShrinkLongestTrimmer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.ShrinkLongestTrimmer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.ShrinkLongestTrimmer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.ShrinkLongestTrimmer.__le__": "text.keras.layers.ToDense.__le__",
    "text.ShrinkLongestTrimmer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.ShrinkLongestTrimmer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.ShrinkLongestTrimmer.__new__": "text.BertTokenizer.__new__",
    "text.ShrinkLongestTrimmer.trim": "text.Trimmer.trim",
    "text.SplitMergeFromLogitsTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitMergeFromLogitsTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitMergeFromLogitsTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitMergeFromLogitsTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitMergeFromLogitsTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitMergeFromLogitsTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitMergeFromLogitsTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SplitMergeFromLogitsTokenizer.split": "text.Tokenizer.split",
    "text.SplitMergeFromLogitsTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.SplitMergeTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitMergeTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitMergeTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitMergeTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitMergeTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitMergeTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitMergeTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SplitMergeTokenizer.split": "text.Tokenizer.split",
    "text.SplitMergeTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.Splitter.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Splitter.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Splitter.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Splitter.__init__": "text.Detokenizer.__init__",
    "text.Splitter.__le__": "text.keras.layers.ToDense.__le__",
    "text.Splitter.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Splitter.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Splitter.__new__": "text.BertTokenizer.__new__",
    "text.SplitterWithOffsets.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitterWithOffsets.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitterWithOffsets.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitterWithOffsets.__init__": "text.Detokenizer.__init__",
    "text.SplitterWithOffsets.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitterWithOffsets.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitterWithOffsets.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitterWithOffsets.__new__": "text.BertTokenizer.__new__",
    "text.SplitterWithOffsets.split": "text.Splitter.split",
    "text.StateBasedSentenceBreaker.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.StateBasedSentenceBreaker.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.StateBasedSentenceBreaker.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.StateBasedSentenceBreaker.__le__": "text.keras.layers.ToDense.__le__",
    "text.StateBasedSentenceBreaker.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.StateBasedSentenceBreaker.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.StateBasedSentenceBreaker.__new__": "text.BertTokenizer.__new__",
    "text.Tokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Tokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Tokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Tokenizer.__init__": "text.Detokenizer.__init__",
    "text.Tokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Tokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Tokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Tokenizer.__new__": "text.BertTokenizer.__new__",
    "text.TokenizerWithOffsets.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.TokenizerWithOffsets.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.TokenizerWithOffsets.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.TokenizerWithOffsets.__init__": "text.Detokenizer.__init__",
    "text.TokenizerWithOffsets.__le__": "text.keras.layers.ToDense.__le__",
    "text.TokenizerWithOffsets.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.TokenizerWithOffsets.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.TokenizerWithOffsets.__new__": "text.BertTokenizer.__new__",
    "text.TokenizerWithOffsets.split": "text.Tokenizer.split",
    "text.TokenizerWithOffsets.tokenize": "text.Tokenizer.tokenize",
    "text.Trimmer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Trimmer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Trimmer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Trimmer.__init__": "text.StateBasedSentenceBreaker.__init__",
    "text.Trimmer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Trimmer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Trimmer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Trimmer.__new__": "text.BertTokenizer.__new__",
    "text.UnicodeCharTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.UnicodeCharTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.UnicodeCharTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.UnicodeCharTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.UnicodeCharTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.UnicodeCharTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.UnicodeCharTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.UnicodeCharTokenizer.split": "text.Tokenizer.split",
    "text.UnicodeCharTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.UnicodeScriptTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.UnicodeScriptTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.UnicodeScriptTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.UnicodeScriptTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.UnicodeScriptTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.UnicodeScriptTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.UnicodeScriptTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.UnicodeScriptTokenizer.split": "text.Tokenizer.split",
    "text.UnicodeScriptTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.WaterfallTrimmer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WaterfallTrimmer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WaterfallTrimmer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WaterfallTrimmer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WaterfallTrimmer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WaterfallTrimmer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WaterfallTrimmer.__new__": "text.BertTokenizer.__new__",
    "text.WaterfallTrimmer.trim": "text.Trimmer.trim",
    "text.WhitespaceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WhitespaceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WhitespaceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WhitespaceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WhitespaceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WhitespaceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WhitespaceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WhitespaceTokenizer.split": "text.Tokenizer.split",
    "text.WhitespaceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.WordpieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WordpieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WordpieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WordpieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WordpieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WordpieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WordpieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WordpieceTokenizer.split": "text.Tokenizer.split",
    "text.WordpieceTokenizer.split_with_offsets": "text.TokenizerWithOffsets.split_with_offsets",
    "text.keras.layers.UnicodeScriptTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.keras.layers.UnicodeScriptTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.keras.layers.UnicodeScriptTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.keras.layers.UnicodeScriptTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.keras.layers.UnicodeScriptTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.keras.layers.UnicodeScriptTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.keras.layers.UnicodeScriptTokenizer.__new__": "text.keras.layers.ToDense.__new__",
    "text.keras.layers.WhitespaceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.keras.layers.WhitespaceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.keras.layers.WhitespaceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.keras.layers.WhitespaceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.keras.layers.WhitespaceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.keras.layers.WhitespaceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.keras.layers.WhitespaceTokenizer.__new__": "text.keras.layers.ToDense.__new__",
    "text.keras.layers.WordpieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.keras.layers.WordpieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.keras.layers.WordpieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.keras.layers.WordpieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.keras.layers.WordpieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.keras.layers.WordpieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.keras.layers.WordpieceTokenizer.__new__": "text.keras.layers.ToDense.__new__"
  },
  "is_fragment": {
    "text": false,
    "text.BertTokenizer": false,
    "text.BertTokenizer.__eq__": true,
    "text.BertTokenizer.__ge__": true,
    "text.BertTokenizer.__gt__": true,
    "text.BertTokenizer.__init__": true,
    "text.BertTokenizer.__le__": true,
    "text.BertTokenizer.__lt__": true,
    "text.BertTokenizer.__ne__": true,
    "text.BertTokenizer.__new__": true,
    "text.BertTokenizer.detokenize": true,
    "text.BertTokenizer.split": true,
    "text.BertTokenizer.split_with_offsets": true,
    "text.BertTokenizer.tokenize": true,
    "text.BertTokenizer.tokenize_with_offsets": true,
    "text.ByteSplitter": false,
    "text.ByteSplitter.__eq__": true,
    "text.ByteSplitter.__ge__": true,
    "text.ByteSplitter.__gt__": true,
    "text.ByteSplitter.__init__": true,
    "text.ByteSplitter.__le__": true,
    "text.ByteSplitter.__lt__": true,
    "text.ByteSplitter.__ne__": true,
    "text.ByteSplitter.__new__": true,
    "text.ByteSplitter.split": true,
    "text.ByteSplitter.split_with_offsets": true,
    "text.Detokenizer": false,
    "text.Detokenizer.__eq__": true,
    "text.Detokenizer.__ge__": true,
    "text.Detokenizer.__gt__": true,
    "text.Detokenizer.__init__": true,
    "text.Detokenizer.__le__": true,
    "text.Detokenizer.__lt__": true,
    "text.Detokenizer.__ne__": true,
    "text.Detokenizer.__new__": true,
    "text.Detokenizer.detokenize": true,
    "text.FastBertNormalizer": false,
    "text.FastBertNormalizer.__eq__": true,
    "text.FastBertNormalizer.__ge__": true,
    "text.FastBertNormalizer.__gt__": true,
    "text.FastBertNormalizer.__init__": true,
    "text.FastBertNormalizer.__le__": true,
    "text.FastBertNormalizer.__lt__": true,
    "text.FastBertNormalizer.__ne__": true,
    "text.FastBertNormalizer.__new__": true,
    "text.FastBertNormalizer.normalize": true,
    "text.FastBertNormalizer.normalize_with_offsets": true,
    "text.FastBertTokenizer": false,
    "text.FastBertTokenizer.__eq__": true,
    "text.FastBertTokenizer.__ge__": true,
    "text.FastBertTokenizer.__gt__": true,
    "text.FastBertTokenizer.__init__": true,
    "text.FastBertTokenizer.__le__": true,
    "text.FastBertTokenizer.__lt__": true,
    "text.FastBertTokenizer.__ne__": true,
    "text.FastBertTokenizer.__new__": true,
    "text.FastBertTokenizer.detokenize": true,
    "text.FastBertTokenizer.split": true,
    "text.FastBertTokenizer.split_with_offsets": true,
    "text.FastBertTokenizer.tokenize": true,
    "text.FastBertTokenizer.tokenize_with_offsets": true,
    "text.FastSentencepieceTokenizer": false,
    "text.FastSentencepieceTokenizer.__eq__": true,
    "text.FastSentencepieceTokenizer.__ge__": true,
    "text.FastSentencepieceTokenizer.__gt__": true,
    "text.FastSentencepieceTokenizer.__init__": true,
    "text.FastSentencepieceTokenizer.__le__": true,
    "text.FastSentencepieceTokenizer.__lt__": true,
    "text.FastSentencepieceTokenizer.__ne__": true,
    "text.FastSentencepieceTokenizer.__new__": true,
    "text.FastSentencepieceTokenizer.detokenize": true,
    "text.FastSentencepieceTokenizer.tokenize": true,
    "text.FastSentencepieceTokenizer.vocab_size": true,
    "text.FastWordpieceTokenizer": false,
    "text.FastWordpieceTokenizer.__eq__": true,
    "text.FastWordpieceTokenizer.__ge__": true,
    "text.FastWordpieceTokenizer.__gt__": true,
    "text.FastWordpieceTokenizer.__init__": true,
    "text.FastWordpieceTokenizer.__le__": true,
    "text.FastWordpieceTokenizer.__lt__": true,
    "text.FastWordpieceTokenizer.__ne__": true,
    "text.FastWordpieceTokenizer.__new__": true,
    "text.FastWordpieceTokenizer.detokenize": true,
    "text.FastWordpieceTokenizer.split": true,
    "text.FastWordpieceTokenizer.split_with_offsets": true,
    "text.FastWordpieceTokenizer.tokenize": true,
    "text.FastWordpieceTokenizer.tokenize_with_offsets": true,
    "text.FirstNItemSelector": false,
    "text.FirstNItemSelector.__eq__": true,
    "text.FirstNItemSelector.__ge__": true,
    "text.FirstNItemSelector.__gt__": true,
    "text.FirstNItemSelector.__init__": true,
    "text.FirstNItemSelector.__le__": true,
    "text.FirstNItemSelector.__lt__": true,
    "text.FirstNItemSelector.__ne__": true,
    "text.FirstNItemSelector.__new__": true,
    "text.FirstNItemSelector.get_selectable": true,
    "text.FirstNItemSelector.get_selection_mask": true,
    "text.FirstNItemSelector.unselectable_ids": true,
    "text.HubModuleSplitter": false,
    "text.HubModuleSplitter.__eq__": true,
    "text.HubModuleSplitter.__ge__": true,
    "text.HubModuleSplitter.__gt__": true,
    "text.HubModuleSplitter.__init__": true,
    "text.HubModuleSplitter.__le__": true,
    "text.HubModuleSplitter.__lt__": true,
    "text.HubModuleSplitter.__ne__": true,
    "text.HubModuleSplitter.__new__": true,
    "text.HubModuleSplitter.split": true,
    "text.HubModuleSplitter.split_with_offsets": true,
    "text.HubModuleTokenizer": false,
    "text.HubModuleTokenizer.__eq__": true,
    "text.HubModuleTokenizer.__ge__": true,
    "text.HubModuleTokenizer.__gt__": true,
    "text.HubModuleTokenizer.__init__": true,
    "text.HubModuleTokenizer.__le__": true,
    "text.HubModuleTokenizer.__lt__": true,
    "text.HubModuleTokenizer.__ne__": true,
    "text.HubModuleTokenizer.__new__": true,
    "text.HubModuleTokenizer.split": true,
    "text.HubModuleTokenizer.split_with_offsets": true,
    "text.HubModuleTokenizer.tokenize": true,
    "text.HubModuleTokenizer.tokenize_with_offsets": true,
    "text.MaskValuesChooser": false,
    "text.MaskValuesChooser.__eq__": true,
    "text.MaskValuesChooser.__ge__": true,
    "text.MaskValuesChooser.__gt__": true,
    "text.MaskValuesChooser.__init__": true,
    "text.MaskValuesChooser.__le__": true,
    "text.MaskValuesChooser.__lt__": true,
    "text.MaskValuesChooser.__ne__": true,
    "text.MaskValuesChooser.__new__": true,
    "text.MaskValuesChooser.get_mask_values": true,
    "text.MaskValuesChooser.mask_token": true,
    "text.MaskValuesChooser.random_token_rate": true,
    "text.MaskValuesChooser.vocab_size": true,
    "text.RandomItemSelector": false,
    "text.RandomItemSelector.__eq__": true,
    "text.RandomItemSelector.__ge__": true,
    "text.RandomItemSelector.__gt__": true,
    "text.RandomItemSelector.__init__": true,
    "text.RandomItemSelector.__le__": true,
    "text.RandomItemSelector.__lt__": true,
    "text.RandomItemSelector.__ne__": true,
    "text.RandomItemSelector.__new__": true,
    "text.RandomItemSelector.get_selectable": true,
    "text.RandomItemSelector.get_selection_mask": true,
    "text.RandomItemSelector.max_selections_per_batch": true,
    "text.RandomItemSelector.selection_rate": true,
    "text.RandomItemSelector.shuffle_fn": true,
    "text.RandomItemSelector.unselectable_ids": true,
    "text.Reduction": false,
    "text.Reduction.MEAN": true,
    "text.Reduction.STRING_JOIN": true,
    "text.Reduction.SUM": true,
    "text.RegexSplitter": false,
    "text.RegexSplitter.__eq__": true,
    "text.RegexSplitter.__ge__": true,
    "text.RegexSplitter.__gt__": true,
    "text.RegexSplitter.__init__": true,
    "text.RegexSplitter.__le__": true,
    "text.RegexSplitter.__lt__": true,
    "text.RegexSplitter.__ne__": true,
    "text.RegexSplitter.__new__": true,
    "text.RegexSplitter.split": true,
    "text.RegexSplitter.split_with_offsets": true,
    "text.RoundRobinTrimmer": false,
    "text.RoundRobinTrimmer.__eq__": true,
    "text.RoundRobinTrimmer.__ge__": true,
    "text.RoundRobinTrimmer.__gt__": true,
    "text.RoundRobinTrimmer.__init__": true,
    "text.RoundRobinTrimmer.__le__": true,
    "text.RoundRobinTrimmer.__lt__": true,
    "text.RoundRobinTrimmer.__ne__": true,
    "text.RoundRobinTrimmer.__new__": true,
    "text.RoundRobinTrimmer.generate_mask": true,
    "text.RoundRobinTrimmer.trim": true,
    "text.SentencepieceTokenizer": false,
    "text.SentencepieceTokenizer.__eq__": true,
    "text.SentencepieceTokenizer.__ge__": true,
    "text.SentencepieceTokenizer.__gt__": true,
    "text.SentencepieceTokenizer.__init__": true,
    "text.SentencepieceTokenizer.__le__": true,
    "text.SentencepieceTokenizer.__lt__": true,
    "text.SentencepieceTokenizer.__ne__": true,
    "text.SentencepieceTokenizer.__new__": true,
    "text.SentencepieceTokenizer.detokenize": true,
    "text.SentencepieceTokenizer.id_to_string": true,
    "text.SentencepieceTokenizer.split": true,
    "text.SentencepieceTokenizer.split_with_offsets": true,
    "text.SentencepieceTokenizer.string_to_id": true,
    "text.SentencepieceTokenizer.tokenize": true,
    "text.SentencepieceTokenizer.tokenize_with_offsets": true,
    "text.SentencepieceTokenizer.vocab_size": true,
    "text.ShrinkLongestTrimmer": false,
    "text.ShrinkLongestTrimmer.__eq__": true,
    "text.ShrinkLongestTrimmer.__ge__": true,
    "text.ShrinkLongestTrimmer.__gt__": true,
    "text.ShrinkLongestTrimmer.__init__": true,
    "text.ShrinkLongestTrimmer.__le__": true,
    "text.ShrinkLongestTrimmer.__lt__": true,
    "text.ShrinkLongestTrimmer.__ne__": true,
    "text.ShrinkLongestTrimmer.__new__": true,
    "text.ShrinkLongestTrimmer.generate_mask": true,
    "text.ShrinkLongestTrimmer.trim": true,
    "text.SplitMergeFromLogitsTokenizer": false,
    "text.SplitMergeFromLogitsTokenizer.__eq__": true,
    "text.SplitMergeFromLogitsTokenizer.__ge__": true,
    "text.SplitMergeFromLogitsTokenizer.__gt__": true,
    "text.SplitMergeFromLogitsTokenizer.__init__": true,
    "text.SplitMergeFromLogitsTokenizer.__le__": true,
    "text.SplitMergeFromLogitsTokenizer.__lt__": true,
    "text.SplitMergeFromLogitsTokenizer.__ne__": true,
    "text.SplitMergeFromLogitsTokenizer.__new__": true,
    "text.SplitMergeFromLogitsTokenizer.split": true,
    "text.SplitMergeFromLogitsTokenizer.split_with_offsets": true,
    "text.SplitMergeFromLogitsTokenizer.tokenize": true,
    "text.SplitMergeFromLogitsTokenizer.tokenize_with_offsets": true,
    "text.SplitMergeTokenizer": false,
    "text.SplitMergeTokenizer.__eq__": true,
    "text.SplitMergeTokenizer.__ge__": true,
    "text.SplitMergeTokenizer.__gt__": true,
    "text.SplitMergeTokenizer.__init__": true,
    "text.SplitMergeTokenizer.__le__": true,
    "text.SplitMergeTokenizer.__lt__": true,
    "text.SplitMergeTokenizer.__ne__": true,
    "text.SplitMergeTokenizer.__new__": true,
    "text.SplitMergeTokenizer.split": true,
    "text.SplitMergeTokenizer.split_with_offsets": true,
    "text.SplitMergeTokenizer.tokenize": true,
    "text.SplitMergeTokenizer.tokenize_with_offsets": true,
    "text.Splitter": false,
    "text.Splitter.__eq__": true,
    "text.Splitter.__ge__": true,
    "text.Splitter.__gt__": true,
    "text.Splitter.__init__": true,
    "text.Splitter.__le__": true,
    "text.Splitter.__lt__": true,
    "text.Splitter.__ne__": true,
    "text.Splitter.__new__": true,
    "text.Splitter.split": true,
    "text.SplitterWithOffsets": false,
    "text.SplitterWithOffsets.__eq__": true,
    "text.SplitterWithOffsets.__ge__": true,
    "text.SplitterWithOffsets.__gt__": true,
    "text.SplitterWithOffsets.__init__": true,
    "text.SplitterWithOffsets.__le__": true,
    "text.SplitterWithOffsets.__lt__": true,
    "text.SplitterWithOffsets.__ne__": true,
    "text.SplitterWithOffsets.__new__": true,
    "text.SplitterWithOffsets.split": true,
    "text.SplitterWithOffsets.split_with_offsets": true,
    "text.StateBasedSentenceBreaker": false,
    "text.StateBasedSentenceBreaker.__eq__": true,
    "text.StateBasedSentenceBreaker.__ge__": true,
    "text.StateBasedSentenceBreaker.__gt__": true,
    "text.StateBasedSentenceBreaker.__init__": true,
    "text.StateBasedSentenceBreaker.__le__": true,
    "text.StateBasedSentenceBreaker.__lt__": true,
    "text.StateBasedSentenceBreaker.__ne__": true,
    "text.StateBasedSentenceBreaker.__new__": true,
    "text.StateBasedSentenceBreaker.break_sentences": true,
    "text.StateBasedSentenceBreaker.break_sentences_with_offsets": true,
    "text.Tokenizer": false,
    "text.Tokenizer.__eq__": true,
    "text.Tokenizer.__ge__": true,
    "text.Tokenizer.__gt__": true,
    "text.Tokenizer.__init__": true,
    "text.Tokenizer.__le__": true,
    "text.Tokenizer.__lt__": true,
    "text.Tokenizer.__ne__": true,
    "text.Tokenizer.__new__": true,
    "text.Tokenizer.split": true,
    "text.Tokenizer.tokenize": true,
    "text.TokenizerWithOffsets": false,
    "text.TokenizerWithOffsets.__eq__": true,
    "text.TokenizerWithOffsets.__ge__": true,
    "text.TokenizerWithOffsets.__gt__": true,
    "text.TokenizerWithOffsets.__init__": true,
    "text.TokenizerWithOffsets.__le__": true,
    "text.TokenizerWithOffsets.__lt__": true,
    "text.TokenizerWithOffsets.__ne__": true,
    "text.TokenizerWithOffsets.__new__": true,
    "text.TokenizerWithOffsets.split": true,
    "text.TokenizerWithOffsets.split_with_offsets": true,
    "text.TokenizerWithOffsets.tokenize": true,
    "text.TokenizerWithOffsets.tokenize_with_offsets": true,
    "text.Trimmer": false,
    "text.Trimmer.__eq__": true,
    "text.Trimmer.__ge__": true,
    "text.Trimmer.__gt__": true,
    "text.Trimmer.__init__": true,
    "text.Trimmer.__le__": true,
    "text.Trimmer.__lt__": true,
    "text.Trimmer.__ne__": true,
    "text.Trimmer.__new__": true,
    "text.Trimmer.generate_mask": true,
    "text.Trimmer.trim": true,
    "text.UnicodeCharTokenizer": false,
    "text.UnicodeCharTokenizer.__eq__": true,
    "text.UnicodeCharTokenizer.__ge__": true,
    "text.UnicodeCharTokenizer.__gt__": true,
    "text.UnicodeCharTokenizer.__init__": true,
    "text.UnicodeCharTokenizer.__le__": true,
    "text.UnicodeCharTokenizer.__lt__": true,
    "text.UnicodeCharTokenizer.__ne__": true,
    "text.UnicodeCharTokenizer.__new__": true,
    "text.UnicodeCharTokenizer.detokenize": true,
    "text.UnicodeCharTokenizer.split": true,
    "text.UnicodeCharTokenizer.split_with_offsets": true,
    "text.UnicodeCharTokenizer.tokenize": true,
    "text.UnicodeCharTokenizer.tokenize_with_offsets": true,
    "text.UnicodeScriptTokenizer": false,
    "text.UnicodeScriptTokenizer.__eq__": true,
    "text.UnicodeScriptTokenizer.__ge__": true,
    "text.UnicodeScriptTokenizer.__gt__": true,
    "text.UnicodeScriptTokenizer.__init__": true,
    "text.UnicodeScriptTokenizer.__le__": true,
    "text.UnicodeScriptTokenizer.__lt__": true,
    "text.UnicodeScriptTokenizer.__ne__": true,
    "text.UnicodeScriptTokenizer.__new__": true,
    "text.UnicodeScriptTokenizer.split": true,
    "text.UnicodeScriptTokenizer.split_with_offsets": true,
    "text.UnicodeScriptTokenizer.tokenize": true,
    "text.UnicodeScriptTokenizer.tokenize_with_offsets": true,
    "text.WaterfallTrimmer": false,
    "text.WaterfallTrimmer.__eq__": true,
    "text.WaterfallTrimmer.__ge__": true,
    "text.WaterfallTrimmer.__gt__": true,
    "text.WaterfallTrimmer.__init__": true,
    "text.WaterfallTrimmer.__le__": true,
    "text.WaterfallTrimmer.__lt__": true,
    "text.WaterfallTrimmer.__ne__": true,
    "text.WaterfallTrimmer.__new__": true,
    "text.WaterfallTrimmer.generate_mask": true,
    "text.WaterfallTrimmer.trim": true,
    "text.WhitespaceTokenizer": false,
    "text.WhitespaceTokenizer.__eq__": true,
    "text.WhitespaceTokenizer.__ge__": true,
    "text.WhitespaceTokenizer.__gt__": true,
    "text.WhitespaceTokenizer.__init__": true,
    "text.WhitespaceTokenizer.__le__": true,
    "text.WhitespaceTokenizer.__lt__": true,
    "text.WhitespaceTokenizer.__ne__": true,
    "text.WhitespaceTokenizer.__new__": true,
    "text.WhitespaceTokenizer.split": true,
    "text.WhitespaceTokenizer.split_with_offsets": true,
    "text.WhitespaceTokenizer.tokenize": true,
    "text.WhitespaceTokenizer.tokenize_with_offsets": true,
    "text.WordShape": false,
    "text.WordShape.BEGINS_WITH_OPEN_QUOTE": true,
    "text.WordShape.BEGINS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_CLOSE_QUOTE": true,
    "text.WordShape.ENDS_WITH_ELLIPSIS": true,
    "text.WordShape.ENDS_WITH_EMOTICON": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_TERMINAL_PUNCT": true,
    "text.WordShape.ENDS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_TERMINAL_PUNCT": true,
    "text.WordShape.HAS_CURRENCY_SYMBOL": true,
    "text.WordShape.HAS_EMOJI": true,
    "text.WordShape.HAS_MATH_SYMBOL": true,
    "text.WordShape.HAS_MIXED_CASE": true,
    "text.WordShape.HAS_NON_LETTER": true,
    "text.WordShape.HAS_NO_DIGITS": true,
    "text.WordShape.HAS_NO_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_NO_QUOTES": true,
    "text.WordShape.HAS_ONLY_DIGITS": true,
    "text.WordShape.HAS_PUNCTUATION_DASH": true,
    "text.WordShape.HAS_QUOTE": true,
    "text.WordShape.HAS_SOME_DIGITS": true,
    "text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_TITLE_CASE": true,
    "text.WordShape.IS_ACRONYM_WITH_PERIODS": true,
    "text.WordShape.IS_EMOTICON": true,
    "text.WordShape.IS_LOWERCASE": true,
    "text.WordShape.IS_MIXED_CASE_LETTERS": true,
    "text.WordShape.IS_NUMERIC_VALUE": true,
    "text.WordShape.IS_PUNCT_OR_SYMBOL": true,
    "text.WordShape.IS_UPPERCASE": true,
    "text.WordShape.IS_WHITESPACE": true,
    "text.WordpieceTokenizer": false,
    "text.WordpieceTokenizer.__eq__": true,
    "text.WordpieceTokenizer.__ge__": true,
    "text.WordpieceTokenizer.__gt__": true,
    "text.WordpieceTokenizer.__init__": true,
    "text.WordpieceTokenizer.__le__": true,
    "text.WordpieceTokenizer.__lt__": true,
    "text.WordpieceTokenizer.__ne__": true,
    "text.WordpieceTokenizer.__new__": true,
    "text.WordpieceTokenizer.detokenize": true,
    "text.WordpieceTokenizer.split": true,
    "text.WordpieceTokenizer.split_with_offsets": true,
    "text.WordpieceTokenizer.tokenize": true,
    "text.WordpieceTokenizer.tokenize_with_offsets": true,
    "text.WordpieceTokenizer.vocab_size": true,
    "text.__version__": true,
    "text.build_fast_bert_normalizer_model": false,
    "text.build_fast_wordpiece_model": false,
    "text.case_fold_utf8": false,
    "text.coerce_to_structurally_valid_utf8": false,
    "text.combine_segments": false,
    "text.find_source_offsets": false,
    "text.gather_with_default": false,
    "text.greedy_constrained_sequence": false,
    "text.keras": false,
    "text.keras.layers": false,
    "text.keras.layers.ToDense": false,
    "text.keras.layers.ToDense.__eq__": true,
    "text.keras.layers.ToDense.__ge__": true,
    "text.keras.layers.ToDense.__gt__": true,
    "text.keras.layers.ToDense.__init__": true,
    "text.keras.layers.ToDense.__le__": true,
    "text.keras.layers.ToDense.__lt__": true,
    "text.keras.layers.ToDense.__ne__": true,
    "text.keras.layers.ToDense.__new__": true,
    "text.keras.layers.UnicodeScriptTokenizer": false,
    "text.keras.layers.UnicodeScriptTokenizer.__eq__": true,
    "text.keras.layers.UnicodeScriptTokenizer.__ge__": true,
    "text.keras.layers.UnicodeScriptTokenizer.__gt__": true,
    "text.keras.layers.UnicodeScriptTokenizer.__init__": true,
    "text.keras.layers.UnicodeScriptTokenizer.__le__": true,
    "text.keras.layers.UnicodeScriptTokenizer.__lt__": true,
    "text.keras.layers.UnicodeScriptTokenizer.__ne__": true,
    "text.keras.layers.UnicodeScriptTokenizer.__new__": true,
    "text.keras.layers.WhitespaceTokenizer": false,
    "text.keras.layers.WhitespaceTokenizer.__eq__": true,
    "text.keras.layers.WhitespaceTokenizer.__ge__": true,
    "text.keras.layers.WhitespaceTokenizer.__gt__": true,
    "text.keras.layers.WhitespaceTokenizer.__init__": true,
    "text.keras.layers.WhitespaceTokenizer.__le__": true,
    "text.keras.layers.WhitespaceTokenizer.__lt__": true,
    "text.keras.layers.WhitespaceTokenizer.__ne__": true,
    "text.keras.layers.WhitespaceTokenizer.__new__": true,
    "text.keras.layers.WordpieceTokenizer": false,
    "text.keras.layers.WordpieceTokenizer.__eq__": true,
    "text.keras.layers.WordpieceTokenizer.__ge__": true,
    "text.keras.layers.WordpieceTokenizer.__gt__": true,
    "text.keras.layers.WordpieceTokenizer.__init__": true,
    "text.keras.layers.WordpieceTokenizer.__le__": true,
    "text.keras.layers.WordpieceTokenizer.__lt__": true,
    "text.keras.layers.WordpieceTokenizer.__ne__": true,
    "text.keras.layers.WordpieceTokenizer.__new__": true,
    "text.keras.layers.WordpieceTokenizer.set_vocabulary": true,
    "text.mask_language_model": false,
    "text.max_spanning_tree": false,
    "text.max_spanning_tree_gradient": false,
    "text.metrics": false,
    "text.metrics.rouge_l": false,
    "text.ngrams": false,
    "text.normalize_utf8": false,
    "text.normalize_utf8_with_offsets_map": false,
    "text.pad_along_dimension": false,
    "text.pad_model_inputs": false,
    "text.regex_split": false,
    "text.regex_split_with_offsets": false,
    "text.sentence_fragments": false,
    "text.sliding_window": false,
    "text.span_alignment": false,
    "text.span_overlaps": false,
    "text.tflite_registrar": false,
    "text.tflite_registrar.AddByteSplit": false,
    "text.tflite_registrar.AddFastBertNormalize": false,
    "text.tflite_registrar.AddFastSentencepieceDetokenize": false,
    "text.tflite_registrar.AddFastSentencepieceTokenize": false,
    "text.tflite_registrar.AddFastWordpieceDetokenize": false,
    "text.tflite_registrar.AddFastWordpieceTokenize": false,
    "text.tflite_registrar.AddNgramsStringJoin": false,
    "text.tflite_registrar.AddRaggedTensorToTensor": false,
    "text.tflite_registrar.AddWhitespaceTokenize": false,
    "text.tflite_registrar.SELECT_TFTEXT_OPS": true,
    "text.viterbi_constrained_sequence": false,
    "text.wordshape": false
  },
  "link_prefix": null,
  "physical_path": {
    "text": "google3.third_party.tensorflow_text",
    "text.BertTokenizer": "google3.third_party.tensorflow_text.python.ops.bert_tokenizer.BertTokenizer",
    "text.BertTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.bert_tokenizer.BertTokenizer.__init__",
    "text.BertTokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.bert_tokenizer.BertTokenizer.detokenize",
    "text.BertTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.bert_tokenizer.BertTokenizer.tokenize",
    "text.BertTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.bert_tokenizer.BertTokenizer.tokenize_with_offsets",
    "text.ByteSplitter": "google3.third_party.tensorflow_text.python.ops.byte_splitter.ByteSplitter",
    "text.ByteSplitter.__init__": "google3.third_party.tensorflow_text.python.ops.byte_splitter.ByteSplitter.__init__",
    "text.ByteSplitter.split": "google3.third_party.tensorflow_text.python.ops.byte_splitter.ByteSplitter.split",
    "text.ByteSplitter.split_with_offsets": "google3.third_party.tensorflow_text.python.ops.byte_splitter.ByteSplitter.split_with_offsets",
    "text.Detokenizer": "google3.third_party.tensorflow_text.python.ops.tokenization.Detokenizer",
    "text.Detokenizer.__init__": "google3.third_party.tensorflow.python.module.module.Module.__init__",
    "text.Detokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.tokenization.Detokenizer.detokenize",
    "text.FastBertNormalizer": "google3.third_party.tensorflow_text.python.ops.fast_bert_normalizer.FastBertNormalizer",
    "text.FastBertNormalizer.__init__": "google3.third_party.tensorflow_text.python.ops.fast_bert_normalizer.FastBertNormalizer.__init__",
    "text.FastBertNormalizer.normalize": "google3.third_party.tensorflow_text.python.ops.fast_bert_normalizer.FastBertNormalizer.normalize",
    "text.FastBertNormalizer.normalize_with_offsets": "google3.third_party.tensorflow_text.python.ops.fast_bert_normalizer.FastBertNormalizer.normalize_with_offsets",
    "text.FastBertTokenizer": "google3.third_party.tensorflow_text.python.ops.fast_bert_tokenizer.FastBertTokenizer",
    "text.FastBertTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.fast_bert_tokenizer.FastBertTokenizer.__init__",
    "text.FastBertTokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.fast_bert_tokenizer.FastBertTokenizer.detokenize",
    "text.FastBertTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.fast_bert_tokenizer.FastBertTokenizer.tokenize",
    "text.FastBertTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.fast_bert_tokenizer.FastBertTokenizer.tokenize_with_offsets",
    "text.FastSentencepieceTokenizer": "google3.third_party.tensorflow_text.python.ops.fast_sentencepiece_tokenizer.FastSentencepieceTokenizer",
    "text.FastSentencepieceTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.fast_sentencepiece_tokenizer.FastSentencepieceTokenizer.__init__",
    "text.FastSentencepieceTokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.fast_sentencepiece_tokenizer.FastSentencepieceTokenizer.detokenize",
    "text.FastSentencepieceTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.fast_sentencepiece_tokenizer.FastSentencepieceTokenizer.tokenize",
    "text.FastSentencepieceTokenizer.vocab_size": "google3.third_party.tensorflow_text.python.ops.fast_sentencepiece_tokenizer.FastSentencepieceTokenizer.vocab_size",
    "text.FastWordpieceTokenizer": "google3.third_party.tensorflow_text.python.ops.fast_wordpiece_tokenizer.FastWordpieceTokenizer",
    "text.FastWordpieceTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.fast_wordpiece_tokenizer.FastWordpieceTokenizer.__init__",
    "text.FastWordpieceTokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.fast_wordpiece_tokenizer.FastWordpieceTokenizer.detokenize",
    "text.FastWordpieceTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.fast_wordpiece_tokenizer.FastWordpieceTokenizer.tokenize",
    "text.FastWordpieceTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.fast_wordpiece_tokenizer.FastWordpieceTokenizer.tokenize_with_offsets",
    "text.FirstNItemSelector": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.FirstNItemSelector",
    "text.FirstNItemSelector.__init__": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.FirstNItemSelector.__init__",
    "text.FirstNItemSelector.get_selectable": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.FirstNItemSelector.get_selectable",
    "text.FirstNItemSelector.get_selection_mask": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.ItemSelector.get_selection_mask",
    "text.HubModuleSplitter": "google3.third_party.tensorflow_text.python.ops.hub_module_splitter.HubModuleSplitter",
    "text.HubModuleSplitter.__init__": "google3.third_party.tensorflow_text.python.ops.hub_module_splitter.HubModuleSplitter.__init__",
    "text.HubModuleSplitter.split": "google3.third_party.tensorflow_text.python.ops.hub_module_splitter.HubModuleSplitter.split",
    "text.HubModuleSplitter.split_with_offsets": "google3.third_party.tensorflow_text.python.ops.hub_module_splitter.HubModuleSplitter.split_with_offsets",
    "text.HubModuleTokenizer": "google3.third_party.tensorflow_text.python.ops.hub_module_tokenizer.HubModuleTokenizer",
    "text.HubModuleTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.hub_module_tokenizer.HubModuleTokenizer.__init__",
    "text.HubModuleTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.hub_module_tokenizer.HubModuleTokenizer.tokenize",
    "text.HubModuleTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.hub_module_tokenizer.HubModuleTokenizer.tokenize_with_offsets",
    "text.MaskValuesChooser": "google3.third_party.tensorflow_text.python.ops.masking_ops.MaskValuesChooser",
    "text.MaskValuesChooser.__init__": "google3.third_party.tensorflow_text.python.ops.masking_ops.MaskValuesChooser.__init__",
    "text.MaskValuesChooser.get_mask_values": "google3.third_party.tensorflow_text.python.ops.masking_ops.MaskValuesChooser.get_mask_values",
    "text.RandomItemSelector": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.RandomItemSelector",
    "text.RandomItemSelector.__init__": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.RandomItemSelector.__init__",
    "text.RandomItemSelector.get_selectable": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.ItemSelector.get_selectable",
    "text.RandomItemSelector.get_selection_mask": "google3.third_party.tensorflow_text.python.ops.item_selector_ops.RandomItemSelector.get_selection_mask",
    "text.Reduction": "google3.third_party.tensorflow_text.python.ops.ngrams_op.Reduction",
    "text.RegexSplitter": "google3.third_party.tensorflow_text.python.ops.regex_split_ops.RegexSplitter",
    "text.RegexSplitter.__init__": "google3.third_party.tensorflow_text.python.ops.regex_split_ops.RegexSplitter.__init__",
    "text.RegexSplitter.split": "google3.third_party.tensorflow_text.python.ops.regex_split_ops.RegexSplitter.split",
    "text.RegexSplitter.split_with_offsets": "google3.third_party.tensorflow_text.python.ops.regex_split_ops.RegexSplitter.split_with_offsets",
    "text.RoundRobinTrimmer": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.RoundRobinTrimmer",
    "text.RoundRobinTrimmer.__init__": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.RoundRobinTrimmer.__init__",
    "text.RoundRobinTrimmer.generate_mask": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.RoundRobinTrimmer.generate_mask",
    "text.SentencepieceTokenizer": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer",
    "text.SentencepieceTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer.__init__",
    "text.SentencepieceTokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer.detokenize",
    "text.SentencepieceTokenizer.id_to_string": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer.id_to_string",
    "text.SentencepieceTokenizer.string_to_id": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer.string_to_id",
    "text.SentencepieceTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer.tokenize",
    "text.SentencepieceTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer.tokenize_with_offsets",
    "text.SentencepieceTokenizer.vocab_size": "google3.third_party.tensorflow_text.python.ops.sentencepiece_tokenizer.SentencepieceTokenizer.vocab_size",
    "text.ShrinkLongestTrimmer": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.ShrinkLongestTrimmer",
    "text.ShrinkLongestTrimmer.__init__": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.ShrinkLongestTrimmer.__init__",
    "text.ShrinkLongestTrimmer.generate_mask": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.ShrinkLongestTrimmer.generate_mask",
    "text.SplitMergeFromLogitsTokenizer": "google3.third_party.tensorflow_text.python.ops.split_merge_from_logits_tokenizer.SplitMergeFromLogitsTokenizer",
    "text.SplitMergeFromLogitsTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.split_merge_from_logits_tokenizer.SplitMergeFromLogitsTokenizer.__init__",
    "text.SplitMergeFromLogitsTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.split_merge_from_logits_tokenizer.SplitMergeFromLogitsTokenizer.tokenize",
    "text.SplitMergeFromLogitsTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.split_merge_from_logits_tokenizer.SplitMergeFromLogitsTokenizer.tokenize_with_offsets",
    "text.SplitMergeTokenizer": "google3.third_party.tensorflow_text.python.ops.split_merge_tokenizer.SplitMergeTokenizer",
    "text.SplitMergeTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.split_merge_tokenizer.SplitMergeTokenizer.__init__",
    "text.SplitMergeTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.split_merge_tokenizer.SplitMergeTokenizer.tokenize",
    "text.SplitMergeTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.split_merge_tokenizer.SplitMergeTokenizer.tokenize_with_offsets",
    "text.Splitter": "google3.third_party.tensorflow_text.python.ops.splitter.Splitter",
    "text.Splitter.split": "google3.third_party.tensorflow_text.python.ops.splitter.Splitter.split",
    "text.SplitterWithOffsets": "google3.third_party.tensorflow_text.python.ops.splitter.SplitterWithOffsets",
    "text.SplitterWithOffsets.split_with_offsets": "google3.third_party.tensorflow_text.python.ops.splitter.SplitterWithOffsets.split_with_offsets",
    "text.StateBasedSentenceBreaker": "google3.third_party.tensorflow_text.python.ops.state_based_sentence_breaker_op.StateBasedSentenceBreaker",
    "text.StateBasedSentenceBreaker.break_sentences": "google3.third_party.tensorflow_text.python.ops.state_based_sentence_breaker_op.StateBasedSentenceBreaker.break_sentences",
    "text.StateBasedSentenceBreaker.break_sentences_with_offsets": "google3.third_party.tensorflow_text.python.ops.state_based_sentence_breaker_op.StateBasedSentenceBreaker.break_sentences_with_offsets",
    "text.Tokenizer": "google3.third_party.tensorflow_text.python.ops.tokenization.Tokenizer",
    "text.Tokenizer.split": "google3.third_party.tensorflow_text.python.ops.tokenization.Tokenizer.split",
    "text.Tokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.tokenization.Tokenizer.tokenize",
    "text.TokenizerWithOffsets": "google3.third_party.tensorflow_text.python.ops.tokenization.TokenizerWithOffsets",
    "text.TokenizerWithOffsets.split_with_offsets": "google3.third_party.tensorflow_text.python.ops.tokenization.TokenizerWithOffsets.split_with_offsets",
    "text.TokenizerWithOffsets.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.tokenization.TokenizerWithOffsets.tokenize_with_offsets",
    "text.Trimmer": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.Trimmer",
    "text.Trimmer.generate_mask": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.Trimmer.generate_mask",
    "text.Trimmer.trim": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.Trimmer.trim",
    "text.UnicodeCharTokenizer": "google3.third_party.tensorflow_text.python.ops.unicode_char_tokenizer.UnicodeCharTokenizer",
    "text.UnicodeCharTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.unicode_char_tokenizer.UnicodeCharTokenizer.__init__",
    "text.UnicodeCharTokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.unicode_char_tokenizer.UnicodeCharTokenizer.detokenize",
    "text.UnicodeCharTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.unicode_char_tokenizer.UnicodeCharTokenizer.tokenize",
    "text.UnicodeCharTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.unicode_char_tokenizer.UnicodeCharTokenizer.tokenize_with_offsets",
    "text.UnicodeScriptTokenizer": "google3.third_party.tensorflow_text.python.ops.unicode_script_tokenizer.UnicodeScriptTokenizer",
    "text.UnicodeScriptTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.unicode_script_tokenizer.UnicodeScriptTokenizer.__init__",
    "text.UnicodeScriptTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.unicode_script_tokenizer.UnicodeScriptTokenizer.tokenize",
    "text.UnicodeScriptTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.unicode_script_tokenizer.UnicodeScriptTokenizer.tokenize_with_offsets",
    "text.WaterfallTrimmer": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.WaterfallTrimmer",
    "text.WaterfallTrimmer.__init__": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.WaterfallTrimmer.__init__",
    "text.WaterfallTrimmer.generate_mask": "google3.third_party.tensorflow_text.python.ops.trimmer_ops.WaterfallTrimmer.generate_mask",
    "text.WhitespaceTokenizer": "google3.third_party.tensorflow_text.python.ops.whitespace_tokenizer.WhitespaceTokenizer",
    "text.WhitespaceTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.whitespace_tokenizer.WhitespaceTokenizer.__init__",
    "text.WhitespaceTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.whitespace_tokenizer.WhitespaceTokenizer.tokenize",
    "text.WhitespaceTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.whitespace_tokenizer.WhitespaceTokenizer.tokenize_with_offsets",
    "text.WordShape": "google3.third_party.tensorflow_text.python.ops.wordshape_ops.WordShape",
    "text.WordpieceTokenizer": "google3.third_party.tensorflow_text.python.ops.wordpiece_tokenizer.WordpieceTokenizer",
    "text.WordpieceTokenizer.__init__": "google3.third_party.tensorflow_text.python.ops.wordpiece_tokenizer.WordpieceTokenizer.__init__",
    "text.WordpieceTokenizer.detokenize": "google3.third_party.tensorflow_text.python.ops.wordpiece_tokenizer.WordpieceTokenizer.detokenize",
    "text.WordpieceTokenizer.tokenize": "google3.third_party.tensorflow_text.python.ops.wordpiece_tokenizer.WordpieceTokenizer.tokenize",
    "text.WordpieceTokenizer.tokenize_with_offsets": "google3.third_party.tensorflow_text.python.ops.wordpiece_tokenizer.WordpieceTokenizer.tokenize_with_offsets",
    "text.WordpieceTokenizer.vocab_size": "google3.third_party.tensorflow_text.python.ops.wordpiece_tokenizer.WordpieceTokenizer.vocab_size",
    "text.build_fast_bert_normalizer_model": "google3.third_party.tensorflow_text.core.pybinds.pywrap_fast_bert_normalizer_model_builder.PyCapsule.build_fast_bert_normalizer_model",
    "text.build_fast_wordpiece_model": "google3.third_party.tensorflow_text.core.pybinds.pywrap_fast_wordpiece_tokenizer_model_builder.PyCapsule.build_fast_wordpiece_model",
    "text.case_fold_utf8": "google3.third_party.tensorflow_text.python.ops.normalize_ops.case_fold_utf8",
    "text.coerce_to_structurally_valid_utf8": "google3.third_party.tensorflow_text.python.ops.string_ops.coerce_to_structurally_valid_utf8",
    "text.combine_segments": "google3.third_party.tensorflow_text.python.ops.segment_combiner_ops.combine_segments",
    "text.find_source_offsets": "google3.third_party.tensorflow_text.python.ops.normalize_ops.find_source_offsets",
    "text.gather_with_default": "google3.third_party.tensorflow_text.python.ops.pointer_ops.gather_with_default",
    "text.greedy_constrained_sequence": "google3.third_party.tensorflow_text.python.ops.greedy_constrained_sequence_op.greedy_constrained_sequence",
    "text.keras": "google3.third_party.tensorflow_text.python.keras",
    "text.keras.layers": "google3.third_party.tensorflow_text.python.keras.layers",
    "text.keras.layers.ToDense": "google3.third_party.tensorflow_text.python.keras.layers.todense.ToDense",
    "text.keras.layers.ToDense.__init__": "google3.third_party.tensorflow_text.python.keras.layers.todense.ToDense.__init__",
    "text.keras.layers.ToDense.__new__": "keras.utils.version_utils.LayerVersionSelector.__new__",
    "text.keras.layers.UnicodeScriptTokenizer": "google3.third_party.tensorflow_text.python.keras.layers.tokenization_layers.UnicodeScriptTokenizer",
    "text.keras.layers.UnicodeScriptTokenizer.__init__": "google3.third_party.tensorflow_text.python.keras.layers.tokenization_layers.UnicodeScriptTokenizer.__init__",
    "text.keras.layers.WhitespaceTokenizer": "google3.third_party.tensorflow_text.python.keras.layers.tokenization_layers.WhitespaceTokenizer",
    "text.keras.layers.WhitespaceTokenizer.__init__": "google3.third_party.tensorflow_text.python.keras.layers.tokenization_layers.WhitespaceTokenizer.__init__",
    "text.keras.layers.WordpieceTokenizer": "google3.third_party.tensorflow_text.python.keras.layers.tokenization_layers.WordpieceTokenizer",
    "text.keras.layers.WordpieceTokenizer.__init__": "google3.third_party.tensorflow_text.python.keras.layers.tokenization_layers.WordpieceTokenizer.__init__",
    "text.keras.layers.WordpieceTokenizer.set_vocabulary": "google3.third_party.tensorflow_text.python.keras.layers.tokenization_layers.WordpieceTokenizer.set_vocabulary",
    "text.mask_language_model": "google3.third_party.tensorflow_text.python.ops.masking_ops.mask_language_model",
    "text.max_spanning_tree": "google3.third_party.tensorflow_text.gen_mst_ops.max_spanning_tree",
    "text.max_spanning_tree_gradient": "google3.third_party.tensorflow_text.python.ops.mst_ops.max_spanning_tree_gradient",
    "text.metrics": "google3.third_party.tensorflow_text.python.metrics",
    "text.metrics.rouge_l": "google3.third_party.tensorflow_text.python.metrics.text_similarity_metric_ops.rouge_l",
    "text.ngrams": "google3.third_party.tensorflow_text.python.ops.ngrams_op.ngrams",
    "text.normalize_utf8": "google3.third_party.tensorflow_text.python.ops.normalize_ops.normalize_utf8",
    "text.normalize_utf8_with_offsets_map": "google3.third_party.tensorflow_text.python.ops.normalize_ops.normalize_utf8_with_offsets_map",
    "text.pad_along_dimension": "google3.third_party.tensorflow_text.python.ops.pad_along_dimension_op.pad_along_dimension",
    "text.pad_model_inputs": "google3.third_party.tensorflow_text.python.ops.pad_model_inputs_ops.pad_model_inputs",
    "text.regex_split": "google3.third_party.tensorflow_text.python.ops.regex_split_ops.regex_split",
    "text.regex_split_with_offsets": "google3.third_party.tensorflow_text.python.ops.regex_split_ops.regex_split_with_offsets",
    "text.sentence_fragments": "google3.third_party.tensorflow_text.python.ops.sentence_breaking_ops.sentence_fragments",
    "text.sliding_window": "google3.third_party.tensorflow_text.python.ops.sliding_window_op.sliding_window",
    "text.span_alignment": "google3.third_party.tensorflow_text.python.ops.pointer_ops.span_alignment",
    "text.span_overlaps": "google3.third_party.tensorflow_text.python.ops.pointer_ops.span_overlaps",
    "text.tflite_registrar": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar",
    "text.tflite_registrar.AddByteSplit": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddByteSplit",
    "text.tflite_registrar.AddFastBertNormalize": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddFastBertNormalize",
    "text.tflite_registrar.AddFastSentencepieceDetokenize": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddFastSentencepieceDetokenize",
    "text.tflite_registrar.AddFastSentencepieceTokenize": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddFastSentencepieceTokenize",
    "text.tflite_registrar.AddFastWordpieceDetokenize": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddFastWordpieceDetokenize",
    "text.tflite_registrar.AddFastWordpieceTokenize": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddFastWordpieceTokenize",
    "text.tflite_registrar.AddNgramsStringJoin": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddNgramsStringJoin",
    "text.tflite_registrar.AddRaggedTensorToTensor": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddRaggedTensorToTensor",
    "text.tflite_registrar.AddWhitespaceTokenize": "google3.third_party.tensorflow_text.core.pybinds.tflite_registrar.PyCapsule.AddWhitespaceTokenize",
    "text.viterbi_constrained_sequence": "google3.third_party.tensorflow_text.python.ops.viterbi_constrained_sequence_op.viterbi_constrained_sequence",
    "text.wordshape": "google3.third_party.tensorflow_text.python.ops.wordshape_ops.wordshape"
  },
  "py_module_names": [
    "text"
  ]
}
