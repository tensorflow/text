{
  "duplicate_of": {
    "text.BertTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.BertTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.BertTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.BertTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.BertTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.BertTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Detokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Detokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Detokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Detokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Detokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Detokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Detokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SentencepieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SentencepieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SentencepieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SentencepieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SentencepieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SentencepieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SentencepieceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.SplitMergeTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.SplitMergeTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.SplitMergeTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.SplitMergeTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.SplitMergeTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.SplitMergeTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.SplitMergeTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.Tokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.Tokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.Tokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.Tokenizer.__init__": "text.Detokenizer.__init__",
    "text.Tokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.Tokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.Tokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.Tokenizer.__new__": "text.BertTokenizer.__new__",
    "text.TokenizerWithOffsets.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.TokenizerWithOffsets.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.TokenizerWithOffsets.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.TokenizerWithOffsets.__init__": "text.Detokenizer.__init__",
    "text.TokenizerWithOffsets.__le__": "text.keras.layers.ToDense.__le__",
    "text.TokenizerWithOffsets.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.TokenizerWithOffsets.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.TokenizerWithOffsets.__new__": "text.BertTokenizer.__new__",
    "text.TokenizerWithOffsets.tokenize": "text.Tokenizer.tokenize",
    "text.UnicodeScriptTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.UnicodeScriptTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.UnicodeScriptTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.UnicodeScriptTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.UnicodeScriptTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.UnicodeScriptTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.UnicodeScriptTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WhitespaceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WhitespaceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WhitespaceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WhitespaceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WhitespaceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WhitespaceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WhitespaceTokenizer.__new__": "text.BertTokenizer.__new__",
    "text.WordShape.name": "text.Reduction.name",
    "text.WordShape.value": "text.Reduction.value",
    "text.WordpieceTokenizer.__eq__": "text.keras.layers.ToDense.__eq__",
    "text.WordpieceTokenizer.__ge__": "text.keras.layers.ToDense.__ge__",
    "text.WordpieceTokenizer.__gt__": "text.keras.layers.ToDense.__gt__",
    "text.WordpieceTokenizer.__le__": "text.keras.layers.ToDense.__le__",
    "text.WordpieceTokenizer.__lt__": "text.keras.layers.ToDense.__lt__",
    "text.WordpieceTokenizer.__ne__": "text.keras.layers.ToDense.__ne__",
    "text.WordpieceTokenizer.__new__": "text.BertTokenizer.__new__"
  },
  "is_fragment": {
    "text": false,
    "text.BertTokenizer": false,
    "text.BertTokenizer.__eq__": true,
    "text.BertTokenizer.__ge__": true,
    "text.BertTokenizer.__gt__": true,
    "text.BertTokenizer.__init__": true,
    "text.BertTokenizer.__le__": true,
    "text.BertTokenizer.__lt__": true,
    "text.BertTokenizer.__ne__": true,
    "text.BertTokenizer.__new__": true,
    "text.BertTokenizer.tokenize": true,
    "text.BertTokenizer.tokenize_with_offsets": true,
    "text.Detokenizer": false,
    "text.Detokenizer.__eq__": true,
    "text.Detokenizer.__ge__": true,
    "text.Detokenizer.__gt__": true,
    "text.Detokenizer.__init__": true,
    "text.Detokenizer.__le__": true,
    "text.Detokenizer.__lt__": true,
    "text.Detokenizer.__ne__": true,
    "text.Detokenizer.__new__": true,
    "text.Detokenizer.detokenize": true,
    "text.Reduction": false,
    "text.Reduction.MEAN": true,
    "text.Reduction.STRING_JOIN": true,
    "text.Reduction.SUM": true,
    "text.Reduction.name": true,
    "text.Reduction.value": true,
    "text.SentencepieceTokenizer": false,
    "text.SentencepieceTokenizer.__eq__": true,
    "text.SentencepieceTokenizer.__ge__": true,
    "text.SentencepieceTokenizer.__gt__": true,
    "text.SentencepieceTokenizer.__init__": true,
    "text.SentencepieceTokenizer.__le__": true,
    "text.SentencepieceTokenizer.__lt__": true,
    "text.SentencepieceTokenizer.__ne__": true,
    "text.SentencepieceTokenizer.__new__": true,
    "text.SentencepieceTokenizer.detokenize": true,
    "text.SentencepieceTokenizer.id_to_string": true,
    "text.SentencepieceTokenizer.tokenize": true,
    "text.SentencepieceTokenizer.tokenize_with_offsets": true,
    "text.SentencepieceTokenizer.vocab_size": true,
    "text.SplitMergeTokenizer": false,
    "text.SplitMergeTokenizer.__eq__": true,
    "text.SplitMergeTokenizer.__ge__": true,
    "text.SplitMergeTokenizer.__gt__": true,
    "text.SplitMergeTokenizer.__init__": true,
    "text.SplitMergeTokenizer.__le__": true,
    "text.SplitMergeTokenizer.__lt__": true,
    "text.SplitMergeTokenizer.__ne__": true,
    "text.SplitMergeTokenizer.__new__": true,
    "text.SplitMergeTokenizer.tokenize": true,
    "text.SplitMergeTokenizer.tokenize_with_offsets": true,
    "text.Tokenizer": false,
    "text.Tokenizer.__eq__": true,
    "text.Tokenizer.__ge__": true,
    "text.Tokenizer.__gt__": true,
    "text.Tokenizer.__init__": true,
    "text.Tokenizer.__le__": true,
    "text.Tokenizer.__lt__": true,
    "text.Tokenizer.__ne__": true,
    "text.Tokenizer.__new__": true,
    "text.Tokenizer.tokenize": true,
    "text.TokenizerWithOffsets": false,
    "text.TokenizerWithOffsets.__eq__": true,
    "text.TokenizerWithOffsets.__ge__": true,
    "text.TokenizerWithOffsets.__gt__": true,
    "text.TokenizerWithOffsets.__init__": true,
    "text.TokenizerWithOffsets.__le__": true,
    "text.TokenizerWithOffsets.__lt__": true,
    "text.TokenizerWithOffsets.__ne__": true,
    "text.TokenizerWithOffsets.__new__": true,
    "text.TokenizerWithOffsets.tokenize": true,
    "text.TokenizerWithOffsets.tokenize_with_offsets": true,
    "text.UnicodeScriptTokenizer": false,
    "text.UnicodeScriptTokenizer.__eq__": true,
    "text.UnicodeScriptTokenizer.__ge__": true,
    "text.UnicodeScriptTokenizer.__gt__": true,
    "text.UnicodeScriptTokenizer.__init__": true,
    "text.UnicodeScriptTokenizer.__le__": true,
    "text.UnicodeScriptTokenizer.__lt__": true,
    "text.UnicodeScriptTokenizer.__ne__": true,
    "text.UnicodeScriptTokenizer.__new__": true,
    "text.UnicodeScriptTokenizer.tokenize": true,
    "text.UnicodeScriptTokenizer.tokenize_with_offsets": true,
    "text.WhitespaceTokenizer": false,
    "text.WhitespaceTokenizer.__eq__": true,
    "text.WhitespaceTokenizer.__ge__": true,
    "text.WhitespaceTokenizer.__gt__": true,
    "text.WhitespaceTokenizer.__init__": true,
    "text.WhitespaceTokenizer.__le__": true,
    "text.WhitespaceTokenizer.__lt__": true,
    "text.WhitespaceTokenizer.__ne__": true,
    "text.WhitespaceTokenizer.__new__": true,
    "text.WhitespaceTokenizer.tokenize": true,
    "text.WhitespaceTokenizer.tokenize_with_offsets": true,
    "text.WordShape": false,
    "text.WordShape.BEGINS_WITH_OPEN_QUOTE": true,
    "text.WordShape.BEGINS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_CLOSE_QUOTE": true,
    "text.WordShape.ENDS_WITH_ELLIPSIS": true,
    "text.WordShape.ENDS_WITH_EMOTICON": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_MULTIPLE_TERMINAL_PUNCT": true,
    "text.WordShape.ENDS_WITH_PUNCT_OR_SYMBOL": true,
    "text.WordShape.ENDS_WITH_SENTENCE_TERMINAL": true,
    "text.WordShape.ENDS_WITH_TERMINAL_PUNCT": true,
    "text.WordShape.HAS_CURRENCY_SYMBOL": true,
    "text.WordShape.HAS_EMOJI": true,
    "text.WordShape.HAS_MATH_SYMBOL": true,
    "text.WordShape.HAS_MIXED_CASE": true,
    "text.WordShape.HAS_NON_LETTER": true,
    "text.WordShape.HAS_NO_DIGITS": true,
    "text.WordShape.HAS_NO_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_NO_QUOTES": true,
    "text.WordShape.HAS_ONLY_DIGITS": true,
    "text.WordShape.HAS_PUNCTUATION_DASH": true,
    "text.WordShape.HAS_QUOTE": true,
    "text.WordShape.HAS_SOME_DIGITS": true,
    "text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL": true,
    "text.WordShape.HAS_TITLE_CASE": true,
    "text.WordShape.IS_ACRONYM_WITH_PERIODS": true,
    "text.WordShape.IS_EMOTICON": true,
    "text.WordShape.IS_LOWERCASE": true,
    "text.WordShape.IS_MIXED_CASE_LETTERS": true,
    "text.WordShape.IS_NUMERIC_VALUE": true,
    "text.WordShape.IS_PUNCT_OR_SYMBOL": true,
    "text.WordShape.IS_UPPERCASE": true,
    "text.WordShape.IS_WHITESPACE": true,
    "text.WordShape.name": true,
    "text.WordShape.value": true,
    "text.WordpieceTokenizer": false,
    "text.WordpieceTokenizer.__eq__": true,
    "text.WordpieceTokenizer.__ge__": true,
    "text.WordpieceTokenizer.__gt__": true,
    "text.WordpieceTokenizer.__init__": true,
    "text.WordpieceTokenizer.__le__": true,
    "text.WordpieceTokenizer.__lt__": true,
    "text.WordpieceTokenizer.__ne__": true,
    "text.WordpieceTokenizer.__new__": true,
    "text.WordpieceTokenizer.tokenize": true,
    "text.WordpieceTokenizer.tokenize_with_offsets": true,
    "text.case_fold_utf8": false,
    "text.coerce_to_structurally_valid_utf8": false,
    "text.gather_with_default": false,
    "text.greedy_constrained_sequence": false,
    "text.keras": false,
    "text.keras.layers": false,
    "text.keras.layers.ToDense": false,
    "text.keras.layers.ToDense.__call__": true,
    "text.keras.layers.ToDense.__eq__": true,
    "text.keras.layers.ToDense.__ge__": true,
    "text.keras.layers.ToDense.__gt__": true,
    "text.keras.layers.ToDense.__init__": true,
    "text.keras.layers.ToDense.__le__": true,
    "text.keras.layers.ToDense.__lt__": true,
    "text.keras.layers.ToDense.__ne__": true,
    "text.keras.layers.ToDense.__new__": true,
    "text.keras.layers.ToDense.activity_regularizer": true,
    "text.keras.layers.ToDense.add_loss": true,
    "text.keras.layers.ToDense.add_metric": true,
    "text.keras.layers.ToDense.add_weight": true,
    "text.keras.layers.ToDense.build": true,
    "text.keras.layers.ToDense.call": true,
    "text.keras.layers.ToDense.compute_mask": true,
    "text.keras.layers.ToDense.compute_output_shape": true,
    "text.keras.layers.ToDense.compute_output_signature": true,
    "text.keras.layers.ToDense.count_params": true,
    "text.keras.layers.ToDense.dtype": true,
    "text.keras.layers.ToDense.dynamic": true,
    "text.keras.layers.ToDense.from_config": true,
    "text.keras.layers.ToDense.get_config": true,
    "text.keras.layers.ToDense.get_weights": true,
    "text.keras.layers.ToDense.input": true,
    "text.keras.layers.ToDense.input_spec": true,
    "text.keras.layers.ToDense.losses": true,
    "text.keras.layers.ToDense.metrics": true,
    "text.keras.layers.ToDense.name": true,
    "text.keras.layers.ToDense.name_scope": true,
    "text.keras.layers.ToDense.non_trainable_weights": true,
    "text.keras.layers.ToDense.output": true,
    "text.keras.layers.ToDense.set_weights": true,
    "text.keras.layers.ToDense.submodules": true,
    "text.keras.layers.ToDense.trainable": true,
    "text.keras.layers.ToDense.trainable_weights": true,
    "text.keras.layers.ToDense.weights": true,
    "text.keras.layers.ToDense.with_name_scope": true,
    "text.max_spanning_tree": false,
    "text.max_spanning_tree_gradient": false,
    "text.metrics": false,
    "text.metrics.rouge_l": false,
    "text.ngrams": false,
    "text.normalize_utf8": false,
    "text.pad_along_dimension": false,
    "text.regex_split": false,
    "text.regex_split_with_offsets": false,
    "text.sentence_fragments": false,
    "text.sliding_window": false,
    "text.span_alignment": false,
    "text.span_overlaps": false,
    "text.viterbi_constrained_sequence": false,
    "text.wordshape": false
  },
  "py_module_names": [
    "text"
  ]
}
