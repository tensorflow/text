{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS3NA-i6nAFC"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SN5USFEIIK3"
      },
      "source": [
        "# Warm start embedding matrix with changing vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6mJg1g3apaz"
      },
      "source": [
        "This tutorial contains an introduction to `tf.keras.utils.warmstart_embedding_matrix`. You will train a simple Keras model for a sentiment classification task with a base vocabulary. You will learn how to warmstart this model training when you have a new vocabulary using which you want to continue to improve the model training.\n",
        "\n",
        "## Embedding matrix\n",
        "\n",
        "Embeddings give us a way to use an efficient, dense representation in which similar vocabulary tokens have a similar encoding. They are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
        "\n",
        "\n",
        "### Vocabulary\n",
        "\n",
        "The set of unique words used in the text corpus is referred to as the vocabulary. We can use the vocabulary to find the number of times each word appears in the corpus. This helps us analyze which words are more common.Vocabulary allows us to represent each piece of text by the specific words that appear in it.\n",
        "\n",
        "### Why warm starting embedding matrix?\n",
        "\n",
        "A model is trained with a set of embeddings that represents a given vocabulary. If the model needs to be updated or improved as and when the vocabulary input extends(or changes or shuffles), previously the model architecture would change (because the embedding layer's `input_dim` would change). As a consequence users could not reuse previously trained embeddings and the training would start from scratch.\n",
        "\n",
        "`tf.keras.utils.warmstart_embedding_matrix` util can be used to warmstart the embedding layer matrix when vocabulary changes between previously saved checkpoint and model. Vocabulary change could mean, the size of the new vocab is different or the vocabulary is reshuffled or new vocabulary has been added to old vocabulary. If the vocabulary size changes, size of the embedding layer matrix also changes. This util remaps the old vocabulary embeddings to the new embedding layer matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUQErGewZxE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E2x6R8T1o7M"
      },
      "outputs": [],
      "source": [
        "# install tf-nightly as `warmstart_embedding_matrix` is only available in nightly\n",
        "! pip install -q tf-nightly\n",
        "# uninstall nightly tensorboardx and reinstall to work with tf-nightly\n",
        "! pip uninstall --yes tb-nightly tensorboardX tensorboard\n",
        "! pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RutaI-Tpev3T"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFctV8-JZOc"
      },
      "source": [
        "### Load Dataset\n",
        "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) through the tutorial. You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. To read more about loading a dataset from scratch, see the [Loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text).  \n",
        "\n",
        "Download the dataset using Keras file utility and take a look at the directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPO4_UmfF0KH"
      },
      "outputs": [],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    \"aclImdb_v1.tar.gz\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
        ")\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
        "os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6yROZNKvbd"
      },
      "source": [
        "Take a look at the `train/` directory. It has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-iOHJGN6SDu"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O59BdioK8jY"
      },
      "source": [
        "The `train` directory also has additional folders which should be removed before creating training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Vfi9oWMSh-"
      },
      "outputs": [],
      "source": [
        "remove_dir = os.path.join(train_dir, \"unsup\")\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFoJjiEyJz9u"
      },
      "source": [
        "Next, create a `tf.data.Dataset` using `tf.keras.utils.text_dataset_from_directory`. You can read more about using this utility in this [text classification tutorial](https://www.tensorflow.org/tutorials/keras/text_classification). \n",
        "\n",
        "Use the `train` directory to create both train and validation datasets with a split of 20% for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItYD3TLkCOP1"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "seed = 123\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        ")\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHV2pchDhzDn"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "You can learn more about `.cache()` and `.prefetch()`, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz6k1IW7h1TO"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGicgV5qT0wh"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6NZSqIIoU0Y"
      },
      "source": [
        "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. You can learn more about using this layer in the [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MlsXzo-ZlfK"
      },
      "outputs": [],
      "source": [
        "# Create a custom standardization function to strip HTML break tags '\u003cbr /\u003e'.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"\u003cbr /\u003e\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 10000\n",
        "sequence_length = 100\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Note that the layer uses the custom standardization defined above.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI9_wLIiWO8Z"
      },
      "source": [
        "## Create a classification model\n",
        "\n",
        "Use the [Keras Functional API](https://www.tensorflow.org/guide/keras/functional) to define the sentiment classification model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHLcFtn5Wsqj"
      },
      "outputs": [],
      "source": [
        "# build a functional model\n",
        "embedding_dim = 16\n",
        "text_model_input = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\n",
        "text_vectorize_layer = vectorize_layer(text_model_input)\n",
        "text_embedding_layer = Embedding(vocab_size, embedding_dim, name=\"embedding\")(\n",
        "    text_vectorize_layer\n",
        ")\n",
        "global_avg_pool = GlobalAveragePooling1D()(text_embedding_layer)\n",
        "dense_1 = Dense(16, activation=\"relu\")(global_avg_pool)\n",
        "output = Dense(1)(dense_1)\n",
        "model = Model(inputs=text_model_input, outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLNgKO7W2fe"
      },
      "source": [
        "## Compile and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpX9etB6IOQd"
      },
      "source": [
        "You will use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize metrics including loss and accuracy. Create a `tf.keras.callbacks.TensorBoard`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Hg3IHFt4Px"
      },
      "outputs": [],
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OrKAKAKIbuH"
      },
      "source": [
        "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCUgdP69Wzix"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mQehiQyv8rP"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    callbacks=[tensorboard_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wYnVedSPfmX"
      },
      "source": [
        "With this approach the model reaches a validation accuracy of around 85% \n",
        "\n",
        "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n",
        "\n",
        "You can look into the model summary to learn more about each layer of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDCgjWyq_0dc"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQbOJZ2WBFY"
      },
      "source": [
        "Visualize the model metrics in TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uanp2YH8RzU"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKp2PvLYI-r2"
      },
      "source": [
        "# Vocabulary Remapping\n",
        "\n",
        "Scenerio: The vocab size has now changed. The new vocab has new words or lesser words or is shuffled, etc. The embedding layer needs to be remapped and updated.\n",
        "\n",
        "Get base vocabulary and embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFgt2n6HJDAw"
      },
      "outputs": [],
      "source": [
        "embedding_weights_base = model.get_layer(\"embedding\").get_weights()[0]\n",
        "vocab_base = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8wuaIVkJaNw"
      },
      "source": [
        "Define a new vectorization layer to generate a new bigger vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-YcdW4XJlcX"
      },
      "outputs": [],
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size_new = 10200\n",
        "sequence_length = 100\n",
        "\n",
        "vectorize_layer_new = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size_new,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer_new.adapt(text_ds)\n",
        "\n",
        "# get new vocab\n",
        "vocab_new = vectorize_layer_new.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHsDOlAnJrFH"
      },
      "source": [
        "Generate updated embeddings using warmstart_embedding_matrix util."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgBlw3VnKrBL"
      },
      "outputs": [],
      "source": [
        "# generate updated embedding matrix\n",
        "updated_embedding = tf.keras.utils.warmstart_embedding_matrix(\n",
        "    base_vocabulary=vocab_base,\n",
        "    new_vocabulary=vocab_new,\n",
        "    base_embeddings=embedding_weights_base,\n",
        "    new_embeddings_initializer=\"uniform\",\n",
        ")\n",
        "# update model variable\n",
        "updated_embedding_variable = tf.Variable(updated_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEhm8fldKyR_"
      },
      "source": [
        "**OR**\n",
        "\n",
        "If you have an embedding matrix which you would like to initialize the new embedding matrix with use `keras.initializers.Constant` as new_embeddings initializer. Uncomment the following block to try this out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwbFxfbMK6Xm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# generate updated embedding matrix\n",
        "new_embedding = np.random.rand(len(vocab_new), 16)\n",
        "updated_embedding = tf.keras.utils.warmstart_embedding_matrix(\n",
        "            base_vocabulary=vocab_base,\n",
        "            new_vocabulary=vocab_new,\n",
        "            base_embeddings=embedding_weights_base,\n",
        "            new_embeddings_initializer=tf.keras.initializers.Constant(\n",
        "                new_embedding\n",
        "            )\n",
        "        )\n",
        "# update model variable\n",
        "updated_embedding_variable = tf.Variable(updated_embedding)\n",
        "''''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbKLjXfhLUVa"
      },
      "source": [
        "verify if embedding matrix shape has changed to reflect new vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYDrBBEtLWZQ"
      },
      "outputs": [],
      "source": [
        "updated_embedding_variable.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCd8LnSILZqk"
      },
      "source": [
        "Now that we have the updated embedding matrix, the next step is to update the layer weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-PdukkBLlx1"
      },
      "outputs": [],
      "source": [
        "model.get_layer(\"embedding\").embeddings = updated_embedding_variable\n",
        "\n",
        "# Verify updated weights shape\n",
        "# The new weights shape should reflect new vocab size\n",
        "model.get_layer(\"embedding\").get_weights()[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juAdUZkVMpEj"
      },
      "source": [
        "modify the model architecture to use the new text vectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etCo20sPNn2C"
      },
      "outputs": [],
      "source": [
        "text_vectorize_layer_new = vectorize_layer_new(text_model_input)\n",
        "text_embedd = Embedding(vocab_size, embedding_dim, name=\"embedding\")(\n",
        "    text_vectorize_layer_new\n",
        ")\n",
        "model = Model(inputs=text_model_input, outputs=output)\n",
        "\n",
        "# view model summary and check updated Param # for Embedding layer.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi4r5FubN202"
      },
      "source": [
        "We have successfully updated the model to accept a new vocabulary. The embedding layer is updated to map old vocabulary words to old embeddings and initialize embeddings for new vocabulary words to be learnt. The learned weights of the rest of the model will remain the same. The model is warmstarted to continue to train from where it left off previously.\n",
        "\n",
        "Let us verify that the remapping worked. Get index of the vocabulary word \"the\" that is present both in base and new vocabulary and compare the embedding values. They should be equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCdlWvpPPEow"
      },
      "outputs": [],
      "source": [
        "# new vocab words\n",
        "example_old_vocab_word = \"the\"  # index 2\n",
        "base_vocab_index = vectorize_layer(\"the\")[0]\n",
        "new_vocab_index = vectorize_layer_new(\"the\")[0]\n",
        "print(\n",
        "    model.get_layer(\"embedding\")(new_vocab_index)\n",
        "    == embedding_weights_base[base_vocab_index]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1xX0XCEReRC"
      },
      "source": [
        "# Continue with warm started training\n",
        "\n",
        "Notice how the training is warmstarted. The accuracy of first epoch is around 85%. Close to the accuracy where the previous traning ended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtbXMQsTRdvq"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    callbacks=[tensorboard_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z67BhOZR6do"
      },
      "source": [
        "# Visualize warm started training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXPXUfw3QTY-"
      },
      "outputs": [],
      "source": [
        "# docs_infra: no_execute\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "warmstart_embedding_matrix.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
