// Copyright 2022 TF.Text Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "tensorflow_text/core/kernels/phrase_tokenizer.h"

#include <iostream>
#include <ostream>
#include <string>
#include <vector>

#include "base/logging.h"
#include "absl/strings/str_join.h"
#include "absl/strings/string_view.h"
#include "icu4c/source/common/unicode/appendable.h"
#include "icu4c/source/common/unicode/bytestream.h"
#include "icu4c/source/common/unicode/edits.h"
#include "icu4c/source/common/unicode/normalizer2.h"
#include "icu4c/source/common/unicode/schriter.h"
#include "icu4c/source/common/unicode/stringoptions.h"
#include "icu4c/source/common/unicode/stringpiece.h"
#include "icu4c/source/common/unicode/uchar.h"
#include "icu4c/source/common/unicode/ucnv.h"
#include "icu4c/source/common/unicode/ucnv_err.h"
#include "icu4c/source/common/unicode/umachine.h"
#include "icu4c/source/common/unicode/uniset.h"
#include "icu4c/source/common/unicode/unistr.h"
#include "icu4c/source/common/unicode/uset.h"
#include "icu4c/source/common/unicode/utf.h"
#include "icu4c/source/common/unicode/utf8.h"
#include "icu4c/source/common/unicode/utypes.h"
#include "tensorflow/lite/kernels/shim/status_macros.h"
#include "tensorflow_text/core/kernels/whitespace_tokenizer_config_builder.h"

namespace tensorflow {
namespace text {

/*static*/ absl::StatusOr<PhraseTokenizer> PhraseTokenizer::Create(
    const void* config_flatbuffer) {
  PhraseTokenizer tokenizer;
  // `GetPhraseTokenizerConfig()` is autogenerated by flatbuffer.
  tokenizer.phrase_config_ = GetPhraseTokenizerConfig(config_flatbuffer);
  std::vector<std::string> vocab_list;
  auto vocab_array = tokenizer.phrase_config_->vocab_array();
  for (int i = 0; i < vocab_array->size(); i++) {
    vocab_list.emplace_back(vocab_array->Get(i)->string_view());
  }
  tokenizer.vocab_ = absl::make_unique<StringVocab>(vocab_list);
  tokenizer.width_ = tokenizer.phrase_config_->width();
  WhitespaceTokenizerConfig whitespace_config(BuildWhitespaceTokenizerConfig());
  tokenizer.whitespace_config_ = absl::make_unique<WhitespaceTokenizerConfig>(
      *std::move(&whitespace_config));
  return std::move(tokenizer);
}

void PhraseTokenizer::Tokenize(const absl::string_view input,
                               std::vector<std::string>* result_tokens,
                               std::vector<int>* result_token_ids) {
  const int input_size = input.size();
  int position = 0, prev_position = 0;
  UChar32 codepoint;
  bool inside_token = false;

  // Word level information.
  std::vector<std::string> tokens;
  std::vector<int> start_offsets;
  std::vector<int> end_offsets;

  // Phrase level. Store the previos width results.
  std::vector<std::vector<std::string>> all_tokens(width_,
                                                   std::vector<std::string>());
  std::vector<std::vector<int>> all_token_ids(width_, std::vector<int>());
  std::vector<int> max_ngram_len(width_, -1);
  int cur_index;
  while (position < input_size) {
    prev_position = position;
    U8_NEXT(input, position, input_size, codepoint);
    if (whitespace_config_->IsWhitespace(codepoint)) {
      if (inside_token) {
        int end_pos = position - 1;
        end_offsets.push_back(end_pos);
        int start_pos = start_offsets.back();
        std::string token(input.substr(start_pos, end_pos - start_pos));
        tokens.push_back(token);
        inside_token = false;
        FindPhraseTokens(tokens, all_tokens, all_token_ids, max_ngram_len,
                         &cur_index);
      }
    } else {
      if (!inside_token) {
        start_offsets.push_back(prev_position);
        inside_token = true;
      }
    }
  }
  // save final word
  if (inside_token) {
    int end_pos = position;
    end_offsets.push_back(end_pos);
    int start_pos = start_offsets.back();
    std::string token(input.substr(start_pos, end_pos - start_pos));
    tokens.push_back(token);
    FindPhraseTokens(tokens, all_tokens, all_token_ids, max_ngram_len,
                     &cur_index);
  }

  result_tokens->insert(result_tokens->end(), all_tokens[cur_index].begin(),
                        all_tokens[cur_index].end());
  result_token_ids->insert(result_token_ids->end(),
                           all_token_ids[cur_index].begin(),
                           all_token_ids[cur_index].end());
}

void PhraseTokenizer::FindPhraseTokens(
    const std::vector<std::string>& tokens,
    std::vector<std::vector<std::string>>& all_tokens,
    std::vector<std::vector<int>>& all_token_ids,
    std::vector<int>& max_ngram_len, int* cur_index) {
  // We find a new word now.
  int cur_word_num = tokens.size();
  // Loops through all width previous results.
  int cur_max_ngram_len = 0;
  for (int j = width_; j >= 1; j--) {
    if (j > cur_word_num) continue;
    // index on array all_*
    int prev_index = (cur_word_num - j + width_) % width_;
    *cur_index = (cur_word_num + width_) % width_;

    auto concat = [](std::string a, std::string b) {
      if (a.empty()) {
        return b;
      }
      if (b.empty()) {
        return a;
      }
      return std::move(a) + ' ' + std::move(b);
    };
    std::string cur_phrase =
        std::accumulate(tokens.begin() + cur_word_num - j,
                        tokens.begin() + cur_word_num, std::string(), concat);
    bool in_vocab = false;
    int token_id = phrase_config_->unk_token_id();
    auto status = PhraseLookup(cur_phrase, &in_vocab, &token_id);
    std::cout << cur_phrase << " is in yunzhu vocab?? " << in_vocab
              << std::endl;
    // Please raise error here.
    if (!status.success) {
      LOG(ERROR) << "Error in phrase lookup.";
      return;
    }
    if (j == 1 || in_vocab) {
      int prev_max_len = max_ngram_len[prev_index];
      int cur_max_len = std::max(prev_max_len, j);
      if (cur_max_len > cur_max_ngram_len) {
        cur_max_ngram_len = cur_max_len;
        const std::vector<std::string>& prev_token = all_tokens[prev_index];
        const std::vector<int>& prev_token_id = all_token_ids[prev_index];
        if (*cur_index != prev_index) {
          all_tokens[*cur_index].clear();
          all_tokens[*cur_index].insert(all_tokens[*cur_index].end(),
                                        prev_token.begin(), prev_token.end());
          all_token_ids[*cur_index].clear();
          all_token_ids[*cur_index].insert(all_token_ids[*cur_index].end(),
                                           prev_token_id.begin(),
                                           prev_token_id.end());
        }
        if (j == 1 && !in_vocab) {
          cur_phrase = phrase_config_->unk_token()->string_view();
        }
        all_tokens[*cur_index].push_back(cur_phrase);
        all_token_ids[*cur_index].push_back(token_id);
        max_ngram_len[*cur_index] = cur_max_ngram_len;
      }
    }
  }
}

LookupStatus PhraseTokenizer::PhraseLookup(const absl::string_view& token,
                                           bool* in_vocab, int* index) {
  auto status = vocab_->Contains(token, in_vocab);
  if (status.success) {
    auto res = vocab_->LookupId(token);
    if (res.has_value()) {
      *index = res.value();
    }
  }
  return status;
}

absl::StatusOr<std::vector<std::string>> PhraseTokenizer::DetokenizeToTokens(
    const absl::Span<const int> input) const {
  std::vector<std::string> output_tokens;
  if (!phrase_config_->support_detokenization()) {
    return absl::FailedPreconditionError(
        "Detokenize function is only enabled when support_detokenization is "
        "true in the config flatbuffer. Please rebuild the model flatbuffer "
        "by setting support_detokenization=true.");
  }
  for (int id : input) {
    auto vocab = phrase_config_->vocab_array()->Get(id);
    output_tokens.emplace_back(vocab->string_view());
  }
  return output_tokens;
}

absl::StatusOr<std::string> PhraseTokenizer::Detokenize(
    const absl::Span<const int> input) const {
  SH_ASSIGN_OR_RETURN(std::vector<std::string> output_tokens,
                      DetokenizeToTokens(input));
  return absl::StrJoin(output_tokens, " ");
}

}  // namespace text
}  // namespace tensorflow
